{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Basics of Feed-Forward Neural Networks\n",
    "----------------------------------------------------------------------\n",
    "Author: Kevin Paganini / (MSOE professor)\n",
    "Date: 3/14/2023\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "In this lab, we will start to create a feed-forward neural network from scratch.\n",
    "We begin with the very basic computational unit, a perceptron,\n",
    "and then we will add more layers and increase the complexity of our network. Along the way, we will learn how a perceptron works, the benefits of adding more layers, the kind of transformations necessary for learning complex features and relationships from data, and why an object-oriented paradigm is useful for easier management of our neural network framework.\n",
    "\n",
    "We will implement everything from scratch in Python using helpful\n",
    "libraries such as NumPy and PyTorch (without using the autograd feature of PyTorch). The purpose of this lab and the following lab\n",
    "series is to learn how neural networks work starting from the most basic\n",
    "computational units and proceeding to deeper and more networks. This will help us better understand how other popular deep learning frameworks, such as PyTorch, work underneath. You should be able to easily understand and implement everything in this lab. If you are having trouble consult with your instructors as the next lab series will assume a perfect understanding of the basic feed-forward neural network material.\n",
    "\n",
    "The recommended Python version for this implementation is 3.7. Recommended reading: sections 4.1 and 4.2 of the book (https://www.d2l.ai/chapter_multilayer-perceptrons/index.html).\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A perceptron or artificial neuron is the most basic processing unit of feed-forward neural networks. A perceptron can be modeled as a single-layer neural network with an input vector $\\mathbf{x} \\in \\mathbb{R}^n$, a bias $b$, a vector of trainable weights $\\mathbf{w} \\in \\mathbb{R}^n$, and an output unit $y$. Given the input $\\mathbf{x}$, the output $y$ is computed by an activation function $f(\\cdot)$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "y (\\mathbf{x}; \\Theta) = f\\left(\\left(\\sum_{i=1}^{n} x_i w_i\\right) + b \\right) = f(\\mathbf{w}^\\intercal \\mathbf{x} + b)\\,,\n",
    "\\end{equation}\n",
    "where $\\Theta = \\{\\mathbf{w}, b\\}$ represents the trainable parameter set. \n",
    "\n",
    "The figure below shows a schematic view of a single output perceptron. Each input value $x_i$ is multiplied by a weight factor $w_i$. The weighted sum added to the bias is then passed through an activation function to obtain the output, $y$.\n",
    "\n",
    "![MLP example](img/perceptron.png)\n",
    " \n",
    "The vector $\\mathbf{x}$ represents one sample of our data and each element $x_i$ represents a feature. Thus, $\\mathbf{x}$ is often referred to as a feature vector. These features can represent different measurements depending on the application. For example, if we are trying to predict if a patient is at high risk of cardiac disease then each element of $\\mathbf{x}$ might contain vital signs such as diastolic and systolic blood pressure, heart rate, blood sugar levels, etc. In another application where we are trying to predict if a tissue biopsy is cancerous or not using mid-infrared imaging then each element of $\\mathbf{x}$ can represent the amount of mid-infrared light absorbed at a particular wavelength. The output $y$ in the applications above could contain values of $0$ or $1$, indicating if the patient is at high risk of cardiac disease or if the tissue biopsy is cancerous or not.\n",
    " \n",
    "Now, let us begin implementing our first artificial neuron.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Let's assume that our feature vector contains measurements of body temperature pressure, pulse oximeter reading, and presence of cough or not. Then for a 'healthy' patient our input sample might look like $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\end{bmatrix}$. Let's say that we are trying to 'predict' the probability of a patient being positive with COVID-19 based on the above measurements.\n",
    "\n",
    "Each element of our input vector is associated with a unique weight. Let the vector of weights be $\\mathbf{w} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\end{bmatrix}$. Each artifical neuron is also associated with a unique bias. Let the bias be $b = 2.9$. Assuming a linear activation function write the code to produce and print the output $y$ given the above input vector $\\mathbf{x}$, weights $\\mathbf{w}$, and the bias $b$ using the above perceptron model. Do not use any NumPy or PyTorch functions. Use a Python variable for each element and use Python lists for vectors.\n",
    "\n",
    "For the activation function, use ReLU. This can be computed as ```x * (x > 0)``` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the neuron is 58.108000000000004\n"
     ]
    }
   ],
   "source": [
    "x = [98.6, 95, 0]\n",
    "w = [0.03, 0.55, 0.88]\n",
    "b = 2.9\n",
    "\n",
    "def perceptron(x, w, b, activation_function):\n",
    "    x_w = [x*w for x, w in zip(x, w)] + [b]\n",
    "    total = sum(x_w)\n",
    "    return activation_function(total)\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return  x * (x > 0)\n",
    "     \n",
    "\n",
    "print(f'The output of the neuron is {perceptron(x, w, b, ReLU)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 1: How many parameters does our simple model contain? Be specific.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Our model contains 4 parameters in this case. We have three weights in the input vector w and one weight in the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 2: Recall that we were hoping to 'predict' the probability of a patient being positive with COVID-19. Does the output make sense? If not, elaborate on how you could fix it.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Since we were told that a healthy person would have a sample as such: [98.6, 95, 0], the model should predict that the person is healthy. It depends on what the model defines as someone who is covid positive. If when the model predicts something positive, as a person who is healthy, then the model makes sense. However, if the model predicts something positive as a person who has covid, then our model is not doing a good job. This confusion comes from the fact that the value being predicted is infinite, it could be any positive number. Using a sigmoid, or a softmax function in output layers can improve interpretability as the values are between 0 and 1, which we can equate to 0 and 100%. Then by setting the threshold of what is covid positive vs not we can tune our precision and recall to our needs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with Multiple Outputs\n",
    "\n",
    "The perceptron model above has only one output. However, in most applications, we need multiple outputs. For example, in a classification problem, we would expect the model to output a vector $\\mathbf{y}$, where each $y_i$ represents the probability of a sample belonging to a particular class $i$. The figure below shows a schematic view of a multiple output feed-forward neural network. Each input value $x_i$ is multiplied by a weight factor $W_{ij}$, where $W_{ij}$ denotes a connection weight between the input node $x_i$ and the output node $y_j$. The weighted sum is added to the bias and then passed through an activation function to obtain the output, $y_j$.\n",
    "\n",
    "![Multi outout perceptron](lab2/img/multi-output-perceptron.png)\n",
    "\n",
    "Given an input $\\mathbf{x} \\in \\mathbb{R}^n$ this can be modeled as:\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f\\left(\\left(\\sum_{i=1}^{n} x_i W_{ij}\\right) + b_j\\right) = f(\\mathbf{w}_j^\\intercal \\mathbf{x} + b_j)\\,,\n",
    "\\end{equation}\n",
    "where the parameter set here is $\\Theta = \\{ \\mathbf{W} \\in \\mathbb{R}^{n \\times m}, \\mathbf{b} \\in \\mathbb{R}^m \\}$ and $\\mathbf{w}_j$ denotes the $j^{th}$ column of $\\mathbf{W}$. \n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "\n",
    "Let $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\\\ 1 \\end{bmatrix}$. Let the output vector $\\mathbf{y} \\in \\mathbb{R}^3$, i.e. consisting of $3$ outputs. Let the weights associated with each output node $y_i$ be $\\mathbf{w_1} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\\\0.73 \\end{bmatrix}$, $\\mathbf{w_2} = \\begin{bmatrix} 0.48 \\\\ 0.31 \\\\ 0.28 \\\\ -0.9 \\end{bmatrix}$, $\\mathbf{w_3} = \\begin{bmatrix} 0.77 \\\\ 0.54 \\\\ 0.32 \\\\ 0.44 \\end{bmatrix}$. Let the bias vector be $\\mathbf{b} = \\begin{bmatrix} 2.9 \\\\ 6.1 \\\\ 3.3 \\end{bmatrix}$. Note that a single bias is associated with each output node $y_i$.\n",
    "\n",
    "Given the above inputs write the code to print the output vector $\\mathbf{y}$.  Use a Python variable for each scalar and use Python lists for vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output vector y is: [58.838, 81.97799999999998, 130.96200000000002]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Your code here\n",
    "import math\n",
    "\n",
    "\n",
    "x = [98.6, 95, 0, 1]\n",
    "w_1 = [0.03, 0.55, 0.88, 0.73]\n",
    "w_2 = [0.48, 0.31, 0.28, -0.9]\n",
    "w_3 = [0.77, 0.54, 0.32, 0.44]\n",
    "b = [2.9, 6.1, 3.3]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "y_1 = perceptron(x, w_1, b[0], ReLU)\n",
    "y_2 = perceptron(x, w_2, b[1], ReLU)\n",
    "y_3 = perceptron(x, w_3, b[2], ReLU)\n",
    "\n",
    "\n",
    "y_vec = [y_1, y_2, y_3]\n",
    "\n",
    "print(f'The output vector y is: {y_vec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Now that you understand how to do basic computations with a simple perceptron model manually, we will proceed to implement the same model above using matrix-vector operations utilizing PyTorch functions. Organizing the computations in matrix-vector format notation makes it simpler to understand and implement neural network models. \n",
    "\n",
    "Write the code to create the same output vector $\\mathbf{y}$ as above by expressing the above computations as matrix-vector multiplications and summation with a bias vector using code vectorization in PyTorch. You should get the same output as above, up to floating-point errors. Again use a Python variable for each scalar, but use PyTorch arrays for vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The resultant output vector is: tensor([ 58.8380,  81.9780, 130.9620])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "weights = torch.tensor([\n",
    "    [0.03, 0.48, 0.77],\n",
    "    [0.55, 0.31, 0.54],\n",
    "    [0.88, 0.28, 0.32],\n",
    "    [0.73, -0.9, 0.44]\n",
    "])\n",
    "\n",
    "inputs = torch.tensor([98.6, 95, 0, 1])\n",
    "\n",
    "bias = torch.tensor([2.9, 6.1, 3.3])\n",
    "\n",
    "\n",
    "mat_mul = inputs@weights\n",
    "mat_mul_bias = mat_mul + bias\n",
    "result = mat_mul_bias.apply_(ReLU)\n",
    "\n",
    "print(f'The resultant output vector is: {result}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "### <font color=blue>Question 3: Explain what each of the dimensions of the matrix of weights $\\mathbf{W}$ and the vector of biases $\\mathbf{b}$ represent?</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Each column in the weight matrix represents the weights of one neuron. Each element of the first column vector is multiplied with the input to produce the output for the first neuron. The first row for instance are the weights that get multiplied with the first feature of the feature vector. The bias vector needs to be reshaped from a column vector to a row vector, so that the element-wise operation matches up shape wise. The bias is what gets added to each neuron in the end before the activation function. The first element of the bias vector gets added to the first node, the second element the second node and so on.     \n",
    "\n",
    "Shapes:    \n",
    "Weights = 4X3    \n",
    "Input = 4X1 (needs to be reshaped) to 1X4\n",
    "\n",
    "WEIGHTS X INPUTS = 1X4 * 4X3 = 1X3    \n",
    "Bias = 3X1 need reshape\n",
    "\n",
    "Element wise addition Bias with 1X3 = 1X3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 4: What is the total number of parameters for this model?</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 12 weights + 3 biases = 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "## More Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "A single-layer perceptron network still represents a linear classifier, even if we were to use nonlinear activation functions. This limitation can be overcome by multi-layer neural networks in combination with nonlinear activation functions, which introduce one or more 'hidden' layers between the input and output layers. Multi-layer neural networks are composed of several simple artificial neurons such that the output of one acts as the input of another. A multi-layer neural network can be represented by a composition function. For a two-layer network with only one output, the composition function can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(2)}\\left(\\sum_{k=1}^{h}W_{kj}^{(2)}*f^{(1)}\\left(\\left(\\sum_{i=1}^{n}W_{ik}^{(1)}*x_i \\right)+b_k^{(1)}\\right)+b_j^{(2)}\\right)\n",
    "\\end{equation}\n",
    "where $h$ is the number of units in the hidden layer and the set of unknown parameters is $\\Theta = \\{\\mathbf{W}^{(1)} \\in R^{n \\times h}, \\mathbf{W}^{(2)} \\in R^{h \\times 1}\\}$. In general, for $L - 1$ hidden layers the composition function, omitting the bias terms, can be written as\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(L)}\\left(\\sum_k W_{kj}^{L}*f^{L-1}\\left(\\sum_{l}W_{lk}^{L - 1}* f^{L - 2}\\left( \\cdots f^{1}\\left(\\sum_{i}W_{iz}^{1}*x_i \\right)\\right) \\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The figure below illustrates a feed-forward neural network composed of an input layer, a hidden layer, and an output layer. In this illustration, the multi-layer neural network has one input layer and one output unit. In most models, the number of hidden layers and output units is more than one.\n",
    "\n",
    "![Feed forward perceptron](img/feed-forward.png)\n",
    "\n",
    "We will now see how to add an additional layer to our model and then how to generalize to any number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add another layer we need another set of weights and biases. \n",
    "\n",
    "W_2 = torch.tensor([\n",
    "       [-0.3, 0.66, 0.98],\n",
    "       [0.58, -0.4, 0.38],\n",
    "       [0.87, 0.69, -0.4]\n",
    "])\n",
    "\n",
    "b_2_old = torch.tensor([3.9, 8.2, 0.8])\n",
    "\n",
    "mat_mul2 = result@W_2\n",
    "\n",
    "mat_mul_bias2 = mat_mul2 + b_2_old\n",
    "\n",
    "result2 = mat_mul_bias2.apply_(ReLU)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 28
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the two layer network is: tensor([147.7328, 104.6057,  37.2281])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write the code to print the output of a 2-layer feed-forward network using the previously computed output,\n",
    "# y, as input to the second layer \n",
    "print(f'The output of the two layer network is: {result2}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "### <font color=blue>Question 5: Explain the dimensions of the weight matrix for the second layer with respect to the dimensions of the previous layer and the number of artificial neurons in the second layer. or Why are the dimensions of the weight matrix of the second layer 3x3?</font>\n",
    "\n",
    "Ans: First dimension of weights correspond to the number of inputs (outputs from previous layer), while the second dimension corresponds to the number of nodes the weights go to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers to Objects\n",
    "\n",
    "Now, we have a feed-forward model (with an input layer, one hidden layer, and one output layer with 3 outputs) capable of processing a batch of data. It would be cumbersome and redundant if we had to keep writing the same code for hundreds of layers. So, to make our code more modular, easier to manage, and less redundant we will represent layers using an object-oriented programming paradigm. Let's define classes for representing our layers.\n",
    "\n",
    "All layer objects should have an `output` instance attribute.  Use good object-oriented practices to avoid code duplication.  To initialize an instance attribute in Python, write `self.attribute_name = attribute_value` in the initializer (`__init__` method).  Don't mention the variable at the top of the class as we would usually do in Java -- this is how you define static attributes in Python.\n",
    "\n",
    "Rather than each layer taking PyTorch arrays as inputs, it should take `Layer`s as inputs, with each layer having its own name. For example, if your network would take $\\mathbf{x}$, $\\mathbf{W}$, and $\\mathbf{b}$ as inputs, you should have attributes `self.x`, `self.W`, and `self.b`.  Then, when you need the values of these inputs, go back and read the output of the previous layer.  For example, if your layer needs the value of $\\mathbf{W}$, you could read `self.W.output` to get it.\n",
    "\n",
    "Two more Python OO hints: (1) `class MyClass1(MyClass2)` is not a constructor call. It is specifying the inheritance relationship. The Java equivalent is `class MyClass1 extends MyClass2`. So you don't want to add arguments on this line. An easy mistake to make.  (2) You must use `self.` every time you access an instance variable in Python. This is how the language was designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the following classes.\n",
    "from functools import reduce\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, output_shape, name=''):\n",
    "        \"\"\"\n",
    "        TODO: Initialize instance attributes here.\n",
    "        \n",
    "        :param output_shape (tuple): the shape of the output array.  When this is a single number, it gives the number of output neurons\n",
    "            When this is an array, it gives the dimensions of the array of output neurons.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        print(f'init layer: {output_shape}')\n",
    "        if isinstance(output_shape, int):\n",
    "            self.nodes = torch.ones(output_shape, 1)\n",
    "        else:\n",
    "            self.nodes = torch.ones(output_shape)\n",
    "       \n",
    "       \n",
    "        \n",
    "\n",
    "class Input(Layer):\n",
    "    def __init__(self, num_outputs, values=None, name=''):\n",
    "        \"\"\"\n",
    "        TODO: Accept any arguments specific to this child class.\n",
    "        num_inputs: How many inputs (features) are there?\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, num_outputs, name)\n",
    "        if values is not None:\n",
    "            print(f'Values shape in init: {values.shape}')\n",
    "            print(f'Self.nodes.shape in init: {self.nodes.shape}')\n",
    "            assert values.shape == self.nodes.shape, \"num_outputs, has to match the shape of values\"\n",
    "            \n",
    "            self.set(values)\n",
    "        else:\n",
    "            self.values = torch.randn(self.nodes.shape)\n",
    "            \n",
    "\n",
    "    def set(self,value):\n",
    "        \"\"\"\n",
    "        TODO: set the `output` of this array to have value `value`.\n",
    "        Raise an error if the size of value is unexpected. An `assert()` is fine for this.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        print(f'Self.nodes.shape: {self.nodes.shape}')\n",
    "        assert value.shape == self.nodes.shape, 'array should have same size as nodes'\n",
    "        self.values = value\n",
    "        print(f'Set value shape: {self.values.shape}')\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"This layer's values do not change during forward propagation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, prev_layer, nodes, weights=None, bias=None):\n",
    "        \"\"\"\n",
    "        TODO: Accept any arguments specific to this child class.\n",
    "        \n",
    "        Raise an error if any of the argument's size do not match as you would expect.\n",
    "        prev_layer: should be a layer object that precedes a certain layer\n",
    "        nodes: an int that represents how many nodes it should have in its layer\n",
    "        weights: A input layer object\n",
    "        bias: A input layer object\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, nodes) \n",
    "        self.prev_layer = prev_layer\n",
    "        if weights is None:\n",
    "            self.weights = torch.randn()\n",
    "            self.weights = Input((prev_layer.nodes.shape[0], self.nodes.shape[0]))\n",
    "            self.weights.set(torch.randn(prev_layer.nodes.shape[0], self.nodes.shape[0]))\n",
    "        else:\n",
    "            assert isinstance(weights, Input), \"weights should be of type Input layer\"\n",
    "            print(f'Weights shape: {weights.nodes.shape}')\n",
    "            print(f'Previous layers shape: {self.prev_layer.nodes.shape}')\n",
    "            print(f'Self node shape: {self.nodes.shape}')\n",
    "            print(f'Predicted shape: {prev_layer.nodes.shape[1]}, {self.nodes.shape[0]}')\n",
    "            assert weights.nodes.shape == (prev_layer.nodes.shape[1], self.nodes.shape[1]), \"Weights shape should be: prev_layer_nodes, curr_layer_nodes\"\n",
    "            self.weights = weights\n",
    "        \n",
    "        if bias is None:\n",
    "            self.bias = Input((self.nodes))\n",
    "            self.bias.set(torch.randn(self.nodes))\n",
    "        else:\n",
    "            \n",
    "            print(f'Bias shape: {bias.values.shape}')\n",
    "            print(f'Nodes shape: {self.nodes.shape}')\n",
    "            \n",
    "            assert bias.nodes.shape == self.nodes.shape, \"bias shape should equal node shape\"\n",
    "            self.bias = bias\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: Set this layer's output based on the outputs of the layers that feed into it.\n",
    "        \"\"\"\n",
    "        temp = self.prev_layer.values\n",
    "        print(f'Previous layer shape: {temp.shape}')\n",
    "        print(f'Weights shape: {self.weights.values.shape}')\n",
    "        print(f'Weights: {self.weights.values}')\n",
    "        mat_mul = temp@self.weights.values\n",
    "        print(f'Mat_mul: {mat_mul}')\n",
    "        print(f'After mat mul: {mat_mul.shape}')\n",
    "        print(f'Bias shape: {self.bias.values.shape}')\n",
    "        print(f'Bias: {self.bias.values}')\n",
    "        if mat_mul.shape[0] == self.bias.values.shape[0]:\n",
    "            mat_mul_bias = mat_mul + self.bias.values\n",
    "        else:\n",
    "            expanded_bias = self.bias.values.repeat(mat_mul.shape[0], 1)\n",
    "            mat_mul_bias = mat_mul + expanded_bias\n",
    "            \n",
    "        print(f'Mat mul plus bias: {mat_mul_bias}')\n",
    "        print(f'After adding bias: {mat_mul_bias.shape}')\n",
    "        self.values = mat_mul_bias.apply_(ReLU)\n",
    "        print(f'Output shape: {self.values.shape}')\n",
    "        print(f'Output: {self.values}')\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init layer: (1, 4)\n",
      "Self.nodes.shape: torch.Size([1, 4])\n",
      "Set value shape: torch.Size([1, 4])\n",
      "Input layer done\n",
      "init layer: (4, 3)\n",
      "Values shape in init: torch.Size([4, 3])\n",
      "Self.nodes.shape in init: torch.Size([4, 3])\n",
      "Self.nodes.shape: torch.Size([4, 3])\n",
      "Set value shape: torch.Size([4, 3])\n",
      "Weight 1 layer done\n",
      "init layer: (1, 3)\n",
      "Values shape in init: torch.Size([1, 3])\n",
      "Self.nodes.shape in init: torch.Size([1, 3])\n",
      "Self.nodes.shape: torch.Size([1, 3])\n",
      "Set value shape: torch.Size([1, 3])\n",
      "Bias 1 layer done\n",
      "init layer: (3, 3)\n",
      "Values shape in init: torch.Size([3, 3])\n",
      "Self.nodes.shape in init: torch.Size([3, 3])\n",
      "Self.nodes.shape: torch.Size([3, 3])\n",
      "Set value shape: torch.Size([3, 3])\n",
      "weight 2 layer done\n",
      "init layer: (1, 3)\n",
      "Values shape in init: torch.Size([1, 3])\n",
      "Self.nodes.shape in init: torch.Size([1, 3])\n",
      "Self.nodes.shape: torch.Size([1, 3])\n",
      "Set value shape: torch.Size([1, 3])\n",
      "bias 2 layer done\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "init layer: (1, 3)\n",
      "Weights shape: torch.Size([4, 3])\n",
      "Previous layers shape: torch.Size([1, 4])\n",
      "Self node shape: torch.Size([1, 3])\n",
      "Predicted shape: 4, 1\n",
      "Bias shape: torch.Size([1, 3])\n",
      "Nodes shape: torch.Size([1, 3])\n",
      "init layer: (1, 3)\n",
      "Weights shape: torch.Size([3, 3])\n",
      "Previous layers shape: torch.Size([1, 3])\n",
      "Self node shape: torch.Size([1, 3])\n",
      "Predicted shape: 3, 1\n",
      "Bias shape: torch.Size([1, 3])\n",
      "Nodes shape: torch.Size([1, 3])\n",
      "\n",
      "Input layer values: tensor([[98.6000, 95.0000,  0.0000,  1.0000]])\n",
      "\n",
      "Previous layer shape: torch.Size([1, 4])\n",
      "Weights shape: torch.Size([4, 3])\n",
      "Weights: tensor([[ 0.0300,  0.4800,  0.7700],\n",
      "        [ 0.5500,  0.3100,  0.5400],\n",
      "        [ 0.8800,  0.2800,  0.3200],\n",
      "        [ 0.7300, -0.9000,  0.4400]])\n",
      "Mat_mul: tensor([[ 55.9380,  75.8780, 127.6620]])\n",
      "After mat mul: torch.Size([1, 3])\n",
      "Bias shape: torch.Size([1, 3])\n",
      "Bias: tensor([[2.9000, 6.1000, 3.3000]])\n",
      "Mat mul plus bias: tensor([[ 58.8380,  81.9780, 130.9620]])\n",
      "After adding bias: torch.Size([1, 3])\n",
      "Output shape: torch.Size([1, 3])\n",
      "Output: tensor([[ 58.8380,  81.9780, 130.9620]])\n",
      "\n",
      "First weight layer outputs: tensor([[ 58.8380,  81.9780, 130.9620]])\n",
      "\n",
      "Previous layer shape: torch.Size([1, 3])\n",
      "Weights shape: torch.Size([3, 3])\n",
      "Weights: tensor([[-0.3000,  0.6600,  0.9800],\n",
      "        [ 0.5800, -0.4000,  0.3800],\n",
      "        [ 0.8700,  0.6900, -0.4000]])\n",
      "Mat_mul: tensor([[143.8328,  96.4057,  36.4281]])\n",
      "After mat mul: torch.Size([1, 3])\n",
      "Bias shape: torch.Size([1, 3])\n",
      "Bias: tensor([[3.9000, 8.2000, 0.8000]])\n",
      "Mat mul plus bias: tensor([[147.7328, 104.6057,  37.2281]])\n",
      "After adding bias: torch.Size([1, 3])\n",
      "Output shape: torch.Size([1, 3])\n",
      "Output: tensor([[147.7328, 104.6057,  37.2281]])\n",
      "\n",
      "Second weight layer outputs: tensor([[147.7328, 104.6057,  37.2281]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input((1,4))\n",
    "input_layer.set(torch.tensor([98.6, 95, 0, 1]).reshape(1,4))\n",
    "print('Input layer done')\n",
    "w_1 = Input((4,3), values=torch.tensor([\n",
    "    [0.03, 0.48, 0.77],\n",
    "    [0.55, 0.31, 0.54],\n",
    "    [0.88, 0.28, 0.32],\n",
    "    [0.73, -0.9, 0.44]]))\n",
    "print('Weight 1 layer done')\n",
    "\n",
    "\n",
    "b_1 = Input((1, 3), torch.tensor([2.9, 6.1, 3.3]).reshape(1, 3))\n",
    "print('Bias 1 layer done')\n",
    "w_2 = Input((3, 3), values=torch.tensor([\n",
    "       [-0.3, 0.66, 0.98],\n",
    "       [0.58, -0.4, 0.38],\n",
    "       [0.87, 0.69, -0.4]]))\n",
    "print('weight 2 layer done')\n",
    "b_2 = Input((1, 3), values=torch.tensor([3.9, 8.2, 0.8]).reshape(1, 3))\n",
    "print('bias 2 layer done')\n",
    "print('\\n\\n\\n')\n",
    "w_layer_1 = Linear(input_layer, (1,3), w_1, bias=b_1)\n",
    "w_layer_2 = Linear(w_layer_1, (1,3), w_2, bias=b_2) \n",
    "print(f'\\nInput layer values: {input_layer.values}\\n')\n",
    "w_layer_1.forward()\n",
    "print(f'\\nFirst weight layer outputs: {w_layer_1.values}\\n')\n",
    "w_layer_2.forward()\n",
    "print(f'\\nSecond weight layer outputs: {w_layer_2.values}\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "weights = torch.tensor([\n",
    "    [0.03, 0.48, 0.77],\n",
    "    [0.55, 0.31, 0.54],\n",
    "    [0.88, 0.28, 0.32],\n",
    "    [0.73, -0.9, 0.44]\n",
    "])\n",
    "\n",
    "inputs = torch.tensor([98.6, 95, 0, 1])\n",
    "\n",
    "bias = torch.tensor([2.9, 6.1, 3.3])\n",
    "\n",
    "tensor([ 58.8380,  81.9780, 130.9620])\n",
    "\n",
    "\n",
    "\n",
    "W_2 = torch.tensor([\n",
    "       [-0.3, 0.66, 0.98],\n",
    "       [0.58, -0.4, 0.38],\n",
    "       [0.87, 0.69, -0.4]\n",
    "])\n",
    "\n",
    "b_2 = torch.tensor([3.9, 8.2, 0.8])\n",
    "\n",
    "tensor([147.7328, 104.6057,  37.2281])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a different network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init layer: (8, 20)\n",
      "Input layer done\n",
      "init layer: (20, 10)\n",
      "Weight 1 layer done\n",
      "init layer: (1, 10)\n",
      "Bias 1 layer done\n",
      "init layer: (10, 100)\n",
      "weight 2 layer done\n",
      "init layer: (1, 100)\n",
      "bias 2 layer done\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "init layer: (1, 10)\n",
      "Weights shape: torch.Size([20, 10])\n",
      "Previous layers shape: torch.Size([8, 20])\n",
      "Self node shape: torch.Size([1, 10])\n",
      "Predicted shape: 20, 1\n",
      "Bias shape: torch.Size([1, 10])\n",
      "Nodes shape: torch.Size([1, 10])\n",
      "init layer: (1, 100)\n",
      "Weights shape: torch.Size([10, 100])\n",
      "Previous layers shape: torch.Size([1, 10])\n",
      "Self node shape: torch.Size([1, 100])\n",
      "Predicted shape: 10, 1\n",
      "Bias shape: torch.Size([1, 100])\n",
      "Nodes shape: torch.Size([1, 100])\n",
      "\n",
      "Input layer values: tensor([[-1.8953, -0.8045, -0.8137, -0.5477,  0.2248, -0.0142, -0.5045,  0.2368,\n",
      "         -0.0717, -0.4759, -0.6889, -2.3197,  0.9233, -0.5495, -1.1696, -0.1345,\n",
      "          0.8383, -3.0031, -0.5398, -1.8714],\n",
      "        [-0.7326,  0.1787, -0.8810,  0.7923, -0.6722, -0.0612,  0.5217,  0.8520,\n",
      "         -0.5931,  0.8966,  0.0712, -0.0427, -0.5287, -0.7201, -0.9102, -1.2515,\n",
      "          0.3422,  1.3305,  0.2847,  0.2943],\n",
      "        [ 1.4204,  0.9992, -1.7693, -0.0310,  0.2193, -1.3378,  1.5330,  0.0921,\n",
      "          0.4729,  1.3130,  0.6845, -0.2577,  1.5193, -0.1023, -0.1688, -0.8573,\n",
      "         -0.9003,  0.8802, -1.1865, -0.5676],\n",
      "        [ 0.1836,  0.9917, -0.7389, -1.0136,  1.7679,  1.1532,  1.9347, -0.5242,\n",
      "         -2.0032,  0.0070,  0.2754, -1.3442, -0.4695,  0.8216, -0.7132, -0.0124,\n",
      "          0.9734, -0.8209,  2.0768,  0.2107],\n",
      "        [ 1.1806,  0.9016, -0.1883,  0.0941, -0.8114,  1.3705,  0.2311,  1.2041,\n",
      "         -1.4889,  1.5941, -0.6912,  2.9868,  0.6075,  0.7653,  0.7294, -0.0356,\n",
      "          0.3704, -0.6825, -0.5380, -1.5805],\n",
      "        [ 0.5082, -0.0042, -1.5500,  0.0751, -1.5655,  1.5852, -0.7228,  0.6436,\n",
      "          1.0221,  0.3148, -1.8974,  0.8305, -1.2723, -1.7786,  1.9922,  0.9706,\n",
      "         -1.9044,  0.9962,  1.5434,  0.3124],\n",
      "        [-0.4401, -0.5614,  2.4502,  2.3775, -0.8733,  1.9873, -2.0533, -0.5088,\n",
      "          0.5445, -0.5206, -0.7456, -0.4043, -0.5365, -0.2831,  0.6004,  1.0338,\n",
      "         -0.1634, -0.8520,  0.3932,  0.9740],\n",
      "        [-1.4785, -0.8217,  1.5750,  0.5830, -0.1089,  0.4651, -1.0763, -0.3240,\n",
      "         -0.5799,  0.1451,  0.8479,  0.8903, -0.5045,  1.3402, -1.3640,  0.7655,\n",
      "         -0.8340, -0.4169,  0.2539,  0.7434]])\n",
      "\n",
      "Previous layer shape: torch.Size([8, 20])\n",
      "Weights shape: torch.Size([20, 10])\n",
      "Weights: tensor([[ 4.7403e-01,  2.4311e-01, -1.2072e+00, -2.7487e-01,  1.4541e+00,\n",
      "          2.2399e-02, -3.2686e-01,  1.6416e+00, -2.3815e-01,  5.6063e-01],\n",
      "        [-9.3226e-01,  7.8042e-01,  7.9832e-01, -2.3985e-01, -4.0424e-02,\n",
      "          1.0037e+00,  4.0409e-01, -1.0536e+00, -1.1867e+00,  3.3863e-02],\n",
      "        [ 7.1453e-01,  2.3103e+00,  5.0789e-01,  4.3136e-01, -1.8194e-01,\n",
      "         -7.7095e-01,  1.2989e+00, -3.3799e-01,  5.6738e-01,  9.9759e-01],\n",
      "        [ 8.3005e-01, -1.5863e+00,  3.0157e-01,  8.3755e-01, -1.2084e+00,\n",
      "         -1.1024e+00,  4.2823e-02, -1.2135e-01, -3.0403e-01, -2.7110e+00],\n",
      "        [-1.4162e+00, -5.0299e-01,  1.4228e+00, -1.1578e+00,  6.4041e-01,\n",
      "         -1.6484e+00,  1.0545e+00, -1.2050e+00, -1.2179e-01, -5.4588e-01],\n",
      "        [-3.5827e-01, -4.7935e-02, -1.3125e+00, -1.0600e+00, -1.5285e+00,\n",
      "          6.8186e-02, -2.3024e-02, -7.9345e-01, -7.5235e-01,  1.0260e+00],\n",
      "        [-5.1019e-01, -1.8974e+00,  7.8553e-01, -1.9289e+00, -8.0386e-01,\n",
      "          6.2755e-01, -2.4812e-01, -9.8134e-01,  8.7313e-01,  1.1561e-01],\n",
      "        [ 2.2767e-01,  9.9010e-01, -3.3866e-01, -1.5495e+00,  4.7246e-01,\n",
      "         -1.4904e+00, -9.0145e-01, -1.1488e-01, -1.2490e+00, -5.9439e-01],\n",
      "        [-8.2447e-01, -2.1232e-01,  9.1439e-01, -7.1745e-01,  2.0251e-01,\n",
      "          9.0377e-01,  1.0350e+00, -2.4924e-01,  9.8305e-01, -1.3931e-02],\n",
      "        [-8.1843e-01,  1.0157e+00,  1.4244e-01,  2.8120e-01, -3.6573e-03,\n",
      "         -1.4783e+00,  3.4873e-01,  1.4706e+00, -9.5152e-01,  6.0858e-01],\n",
      "        [-4.2610e+00, -2.4192e+00,  3.2696e-01, -4.4692e-02, -2.9612e-01,\n",
      "         -1.0939e+00, -9.0854e-01, -3.1974e-01, -1.7811e+00,  8.7947e-02],\n",
      "        [-1.2271e-01, -1.8983e+00, -3.9585e-01, -2.7967e-01, -3.9465e-01,\n",
      "         -9.9966e-01, -8.1622e-01, -1.4371e-01,  4.7149e-01, -8.4733e-01],\n",
      "        [ 3.1169e-01, -8.7055e-01, -1.0969e+00,  5.8019e-01,  1.6104e+00,\n",
      "          4.3312e-01,  1.6414e+00,  2.5480e+00, -6.0404e-01, -5.5553e-01],\n",
      "        [-5.5526e-01,  1.0222e+00,  2.7753e-01,  2.2908e+00,  4.6142e-01,\n",
      "          7.1643e-02,  5.6166e-01,  2.7148e+00, -1.0934e+00, -8.7463e-01],\n",
      "        [ 5.0369e-01, -5.9730e-01, -9.8697e-01,  1.5263e-01,  6.8288e-01,\n",
      "         -1.1599e-01, -7.6695e-01,  2.1169e-01,  1.4488e-02,  1.2688e+00],\n",
      "        [ 1.0205e+00,  1.3718e+00,  7.4539e-01,  1.7915e-01,  1.0231e+00,\n",
      "          1.8222e+00, -1.3325e+00, -5.2449e-01,  1.1377e-02, -1.2302e-02],\n",
      "        [ 8.4297e-01, -1.2937e+00,  9.2663e-01,  3.3408e-01, -6.2999e-01,\n",
      "          1.0824e-01,  8.1571e-01, -8.9122e-01, -5.8941e-01, -1.3365e-01],\n",
      "        [-1.4939e+00, -2.0403e-01, -7.3353e-02,  1.1910e-01,  4.0770e-01,\n",
      "         -4.0968e-02,  8.0438e-01, -6.8483e-02,  3.0549e+00,  2.1943e+00],\n",
      "        [ 3.7581e-01,  1.4487e-01, -1.8574e-01, -1.0543e+00,  4.4548e-01,\n",
      "          6.0512e-01, -1.8851e-01,  2.0165e-01,  4.9027e-01,  6.5748e-01],\n",
      "        [-9.4540e-01, -8.0837e-01,  1.2325e+00,  4.5567e-01, -1.6530e+00,\n",
      "          1.2642e+00, -5.5265e-01, -7.7901e-01, -8.0894e-01,  5.2506e-01]])\n",
      "Mat_mul: tensor([[ 9.1080,  4.6927,  0.1687, -0.4247,  1.2737,  0.8238,  3.3295, -0.3512,\n",
      "         -7.5209, -8.6908],\n",
      "        [-3.4977, -4.5502,  0.5642, -2.5676, -5.4693, -3.6949, -0.5081, -2.8968,\n",
      "          2.0015,  0.0226],\n",
      "        [-8.9028, -6.9409, -1.1840, -2.0194,  6.1136, -1.6608,  2.1936,  7.7463,\n",
      "         -0.1614, -1.1343],\n",
      "        [-3.9390, -1.4006,  3.6536, -6.2097, -2.0516,  2.6458, -0.1760, -4.3067,\n",
      "         -6.0298,  2.2782],\n",
      "        [ 5.6982,  0.0688, -9.1503, -0.5559,  1.7426, -7.2647, -3.4442,  7.7106,\n",
      "         -6.5441, -3.0028],\n",
      "        [ 8.0228,  3.9194, -8.5253, -7.3446,  0.0910,  6.0400, -9.1770, -3.1478,\n",
      "          8.4437,  8.2540],\n",
      "        [10.4086,  7.8988, -1.1807,  5.7568, -6.8366,  1.4908,  0.2836, -3.3298,\n",
      "         -1.3874, -1.5168],\n",
      "        [-2.7201,  4.2703,  2.9875,  6.6015, -4.9992, -2.8015, -0.9147,  0.5567,\n",
      "         -3.0572, -3.7226]])\n",
      "After mat mul: torch.Size([8, 10])\n",
      "Bias shape: torch.Size([1, 10])\n",
      "Bias: tensor([[-0.0226, -0.9649, -1.3543,  0.2322, -1.7335,  0.7567,  0.5605, -1.2753,\n",
      "          0.7194,  0.7582]])\n",
      "Mat mul plus bias: tensor([[ 9.0853e+00,  3.7278e+00, -1.1856e+00, -1.9251e-01, -4.5978e-01,\n",
      "          1.5805e+00,  3.8899e+00, -1.6265e+00, -6.8015e+00, -7.9326e+00],\n",
      "        [-3.5203e+00, -5.5151e+00, -7.9016e-01, -2.3354e+00, -7.2028e+00,\n",
      "         -2.9383e+00,  5.2395e-02, -4.1721e+00,  2.7209e+00,  7.8079e-01],\n",
      "        [-8.9254e+00, -7.9059e+00, -2.5383e+00, -1.7872e+00,  4.3801e+00,\n",
      "         -9.0418e-01,  2.7540e+00,  6.4710e+00,  5.5804e-01, -3.7612e-01],\n",
      "        [-3.9617e+00, -2.3656e+00,  2.2992e+00, -5.9775e+00, -3.7851e+00,\n",
      "          3.4025e+00,  3.8450e-01, -5.5820e+00, -5.3104e+00,  3.0364e+00],\n",
      "        [ 5.6756e+00, -8.9615e-01, -1.0505e+01, -3.2369e-01,  9.1465e-03,\n",
      "         -6.5080e+00, -2.8837e+00,  6.4353e+00, -5.8247e+00, -2.2446e+00],\n",
      "        [ 8.0001e+00,  2.9544e+00, -9.8797e+00, -7.1124e+00, -1.6425e+00,\n",
      "          6.7966e+00, -8.6165e+00, -4.4231e+00,  9.1631e+00,  9.0122e+00],\n",
      "        [ 1.0386e+01,  6.9338e+00, -2.5351e+00,  5.9891e+00, -8.5700e+00,\n",
      "          2.2474e+00,  8.4406e-01, -4.6050e+00, -6.6802e-01, -7.5863e-01],\n",
      "        [-2.7428e+00,  3.3054e+00,  1.6332e+00,  6.8337e+00, -6.7327e+00,\n",
      "         -2.0448e+00, -3.5424e-01, -7.1858e-01, -2.3378e+00, -2.9644e+00]])\n",
      "After adding bias: torch.Size([8, 10])\n",
      "Output shape: torch.Size([8, 10])\n",
      "Output: tensor([[9.0853e+00, 3.7278e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5805e+00,\n",
      "         3.8899e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         5.2395e-02, -0.0000e+00, 2.7209e+00, 7.8079e-01],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.3801e+00, -0.0000e+00,\n",
      "         2.7540e+00, 6.4710e+00, 5.5804e-01, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, 2.2992e+00, -0.0000e+00, -0.0000e+00, 3.4025e+00,\n",
      "         3.8450e-01, -0.0000e+00, -0.0000e+00, 3.0364e+00],\n",
      "        [5.6756e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.1465e-03, -0.0000e+00,\n",
      "         -0.0000e+00, 6.4353e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [8.0001e+00, 2.9544e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.7966e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 9.1631e+00, 9.0122e+00],\n",
      "        [1.0386e+01, 6.9338e+00, -0.0000e+00, 5.9891e+00, -0.0000e+00, 2.2474e+00,\n",
      "         8.4406e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, 3.3054e+00, 1.6332e+00, 6.8337e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00]])\n",
      "\n",
      "First weight layer outputs: tensor([[9.0853e+00, 3.7278e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5805e+00,\n",
      "         3.8899e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         5.2395e-02, -0.0000e+00, 2.7209e+00, 7.8079e-01],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.3801e+00, -0.0000e+00,\n",
      "         2.7540e+00, 6.4710e+00, 5.5804e-01, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, 2.2992e+00, -0.0000e+00, -0.0000e+00, 3.4025e+00,\n",
      "         3.8450e-01, -0.0000e+00, -0.0000e+00, 3.0364e+00],\n",
      "        [5.6756e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.1465e-03, -0.0000e+00,\n",
      "         -0.0000e+00, 6.4353e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [8.0001e+00, 2.9544e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.7966e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 9.1631e+00, 9.0122e+00],\n",
      "        [1.0386e+01, 6.9338e+00, -0.0000e+00, 5.9891e+00, -0.0000e+00, 2.2474e+00,\n",
      "         8.4406e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, 3.3054e+00, 1.6332e+00, 6.8337e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00]])\n",
      "\n",
      "Previous layer shape: torch.Size([8, 10])\n",
      "Weights shape: torch.Size([10, 100])\n",
      "Weights: tensor([[ 2.0689,  0.4086, -0.8701,  0.4523, -1.3990,  0.0441, -1.1255,  0.3737,\n",
      "         -0.5885,  1.1930, -1.7859,  1.8642, -0.6328,  0.1893,  0.1539, -0.7658,\n",
      "         -0.0572, -1.2012,  0.0148, -2.1975, -1.1393, -1.0012, -0.2826, -0.6822,\n",
      "         -1.4270, -0.5447,  0.7472,  0.1993,  0.6701, -0.5602, -1.1733, -2.0769,\n",
      "          2.0873,  0.4755, -0.7072,  0.0194, -0.2313,  0.3629,  0.9902, -1.2307,\n",
      "          1.2903,  1.4507, -0.2393, -0.0287,  1.8707,  0.6621, -0.7180, -1.1082,\n",
      "         -2.5465, -0.1340, -0.5262, -1.0336,  0.5072,  0.5401, -0.0063, -1.1337,\n",
      "         -0.4875,  0.7314,  1.7249, -0.1371, -0.9380, -1.2975, -0.9318,  1.7508,\n",
      "         -1.5370,  0.6696, -1.7427,  0.0660,  0.2346, -0.2952,  0.7273,  1.1130,\n",
      "         -0.3308,  0.0176,  1.4777, -0.4741, -1.2191,  0.9599,  0.3276,  0.1731,\n",
      "          0.9185, -0.2651, -0.1626,  0.2923,  1.3361,  0.8052,  0.4018,  0.1927,\n",
      "         -0.4252, -0.9205, -0.0070,  0.3668, -0.4136, -0.4082, -1.0455,  0.6234,\n",
      "          0.1231,  0.9778,  0.3808,  0.2643],\n",
      "        [-0.3314,  0.7969, -0.8307,  1.1058, -2.3622, -0.0981, -1.4221, -0.3829,\n",
      "          1.1660,  0.0324,  1.4893,  0.1850,  0.6820,  0.6499,  1.4616, -0.1487,\n",
      "          0.8257, -1.6206, -1.9033, -0.8677,  1.1427,  0.3609,  0.1520, -0.0320,\n",
      "          0.0638,  0.7278, -1.2038, -1.2007, -0.0470,  0.7400,  0.3516,  0.0190,\n",
      "         -0.7590,  1.4153, -1.0345,  0.1242, -0.2170, -1.8755,  0.2441,  0.6167,\n",
      "          0.3298,  0.2138,  0.1364, -0.6985, -1.2654, -0.7192, -0.4967,  1.6248,\n",
      "          0.9980,  0.1594,  0.7393,  0.2647,  0.1992,  1.6449,  0.5565,  1.3387,\n",
      "          0.2214, -0.1351, -1.9153,  0.3724,  0.2635,  1.1499, -0.6056, -0.8357,\n",
      "         -0.0177,  0.1163, -1.2454,  1.0253,  0.1965, -1.5641,  0.2445,  1.1359,\n",
      "          0.6949, -1.4600,  0.4511,  0.1324,  0.5574, -1.4553,  1.6233,  0.1661,\n",
      "          1.5433, -1.4754,  0.5464,  2.1133,  0.0773, -0.1814, -0.6482, -2.1285,\n",
      "          0.6296,  0.6953, -1.4240, -0.4236, -0.1425,  2.1702, -0.7955,  0.0279,\n",
      "          0.2079,  0.4300,  1.2530,  0.0678],\n",
      "        [-0.4418, -0.3835, -0.3095,  0.3248, -2.0903, -1.1392,  1.1379,  0.4902,\n",
      "          0.8171, -1.7715, -0.0812, -0.6099, -0.3006, -0.5764, -0.7812,  0.0884,\n",
      "          0.5214, -0.4146,  1.2183,  0.6652,  1.0118,  1.2409, -1.6366, -0.4620,\n",
      "         -0.3157,  0.5346, -2.6065, -0.2749, -0.0731, -1.0385, -1.7466,  2.3479,\n",
      "         -0.2324,  0.3141, -0.4190, -0.1233, -0.2930, -1.1928,  0.4525,  2.1676,\n",
      "          0.0319, -0.1738,  0.5878, -0.9249,  0.1560, -0.1310,  0.2781,  0.4758,\n",
      "          2.4937, -0.0159,  0.1602, -1.1260,  0.3462,  0.0104,  0.6501, -0.6260,\n",
      "         -0.4270, -1.1218,  0.8244,  0.7409,  0.3473,  2.9874, -0.2191,  0.2095,\n",
      "         -0.7593, -1.9410, -0.8990,  0.2093,  0.4605, -0.8538,  0.5004,  0.8386,\n",
      "          1.4021,  1.0578, -0.1327, -1.3021, -0.0456,  0.6357,  0.9713, -0.0803,\n",
      "          0.6905, -1.5357,  0.4793,  1.2855, -0.1357, -1.3296, -0.8016, -0.9305,\n",
      "          0.3147, -0.0861, -0.1199, -2.4831, -2.5045,  2.3980, -2.1981, -1.5386,\n",
      "          1.2240, -0.1664, -0.2979, -0.4762],\n",
      "        [ 0.1700,  0.1724, -0.2909, -1.9915,  0.1142,  0.3245,  0.7194, -0.0668,\n",
      "          1.0264,  1.1994,  0.5219,  0.3146,  0.4449,  0.3746,  1.0080, -1.2765,\n",
      "          0.4762, -0.1809,  0.3696, -0.2577, -0.3474, -1.5500,  0.3285, -0.3328,\n",
      "         -1.2404, -0.0431,  0.1725, -0.4964, -0.3841,  1.1023,  0.0317,  1.0859,\n",
      "          0.5296,  0.2544,  0.9292,  0.0742, -1.7377,  0.0923, -1.6637,  0.0393,\n",
      "         -0.0903,  1.6852, -0.0218, -1.3829,  1.4020, -2.2338,  0.6203,  1.3351,\n",
      "          0.4285,  0.2247, -1.0907, -1.0995, -0.6748,  0.4344, -0.5867,  1.0146,\n",
      "         -0.9029,  1.8648,  0.6989,  0.7475,  0.6195, -1.8390,  1.9305, -0.2806,\n",
      "         -0.4945,  0.0925, -0.9346,  0.5977, -0.2503,  1.5206,  1.5679,  0.0635,\n",
      "          1.4683, -0.3731,  1.5210,  0.2532, -0.4405, -0.6084,  0.4811,  1.1640,\n",
      "          0.5889,  0.0846, -1.1507,  1.3777,  1.3010,  0.2323,  1.0549, -0.4908,\n",
      "          0.3285, -0.1811,  1.0107, -0.3051, -1.2069,  0.6341,  0.2622,  0.6724,\n",
      "          0.2997, -1.5127, -0.5551,  1.1178],\n",
      "        [-1.0789, -0.1804,  1.5545,  0.3062, -0.4621,  0.9617,  1.2323, -0.5831,\n",
      "          0.1095, -0.0467, -0.7276,  0.5968, -0.0095,  0.9827,  0.6628, -0.2810,\n",
      "          0.9159, -0.6299, -0.2107,  1.4278,  1.5802, -1.8094,  0.9992, -1.4991,\n",
      "         -2.0156, -0.2751, -0.0567,  0.5144,  0.8084,  0.2947, -1.0463, -0.5733,\n",
      "         -0.1210, -1.0882,  0.0055, -0.4614,  0.1307,  0.4810, -2.0913,  0.9115,\n",
      "         -2.5827, -0.3148,  1.3416, -0.5761, -0.6761, -0.3241,  0.4845, -0.3338,\n",
      "         -1.8478,  0.1419,  0.3236, -0.1412, -0.2153, -0.0579,  1.4060,  0.9420,\n",
      "         -0.7919, -0.0586,  0.7861,  1.3925,  1.4781, -0.5232,  0.5057, -1.4659,\n",
      "          0.3852, -1.5784, -1.3567, -0.4078, -1.5978, -0.3391, -0.6516, -0.7068,\n",
      "          0.0363, -0.1696, -0.0767, -0.7243,  1.3016, -1.7861,  0.4083,  1.1346,\n",
      "         -0.6305, -0.9967,  0.3782,  0.7369, -0.8480, -0.9947, -0.2816, -0.1222,\n",
      "          0.1199,  1.0015,  0.5718, -1.3511,  1.1980, -1.4915, -1.3984, -0.4100,\n",
      "         -3.0011,  0.2727,  0.9801, -0.2320],\n",
      "        [ 0.2922,  0.9503,  1.3482, -1.3176,  0.2762,  2.7121, -0.7221,  1.4498,\n",
      "          0.5303,  2.6126, -0.9218, -1.0257, -2.0102, -1.2620, -0.0706, -0.1647,\n",
      "          0.1982,  0.3214,  0.2428,  0.5653, -1.6104, -0.8556,  0.7655,  1.0005,\n",
      "          0.9042,  1.2200,  0.2377,  0.6531,  0.6206,  0.7203,  0.8028, -0.3116,\n",
      "          0.7466,  1.2067, -0.6031,  1.4189,  1.3547,  0.6204, -0.5463,  1.4179,\n",
      "         -1.0306,  1.4043,  0.6853, -1.7888, -0.6306,  0.1199,  0.7104, -0.0645,\n",
      "          1.4304, -1.4348,  2.4697,  1.8753, -0.0691, -0.5801,  0.0918, -1.0047,\n",
      "         -1.6342,  1.1513, -0.0231, -1.1111,  0.6815,  0.4709, -0.6914, -0.4543,\n",
      "          0.6031, -1.4067, -0.5853, -0.6864, -0.6280, -0.3720, -1.1902,  0.6782,\n",
      "          0.6898,  0.6719,  0.7780, -0.4168, -0.5857, -0.7411, -1.2974,  1.4537,\n",
      "          0.6365, -0.2016, -0.4283, -0.8033, -0.7998, -0.6040,  1.3733, -0.7883,\n",
      "         -1.5479, -1.3663,  0.3352,  2.5482,  1.4220, -1.1810,  0.9861,  0.5878,\n",
      "         -1.5390,  0.7833,  1.2231, -1.4597],\n",
      "        [ 0.5359, -0.3513, -0.9657, -0.0791,  0.0161,  0.3788,  0.8916, -0.6207,\n",
      "          0.8225,  0.2172, -0.0972, -0.1578,  0.7858,  1.6230, -0.2472, -1.0500,\n",
      "          0.4181,  1.2549,  1.6076,  0.3862,  1.3406, -0.6116,  1.8822, -0.9868,\n",
      "          1.4685, -0.3630, -0.0768, -0.8529,  0.8445, -2.2967,  0.6394, -0.5030,\n",
      "          0.3276,  0.0892, -1.9571, -1.9764,  1.9492,  1.2591, -0.1279, -0.0612,\n",
      "          0.8654, -2.0217, -1.1261,  1.2232,  0.7953,  1.4249,  0.0065, -0.1571,\n",
      "         -0.6377, -1.8234, -0.3159,  0.2457,  0.4504, -0.2987,  1.4886, -0.0870,\n",
      "         -0.0814, -0.4022,  0.6990, -1.0083, -0.3820,  1.4258,  1.2371, -0.5754,\n",
      "         -0.2354, -1.1061,  0.2304, -1.6462, -2.3641,  0.8906,  0.6428, -0.2927,\n",
      "         -0.0966, -0.5773,  0.9624, -0.2396,  0.1348,  1.2160, -0.8608, -1.1220,\n",
      "         -0.1148, -0.2332, -1.2335,  0.3337, -0.7779, -0.3139,  0.7187,  0.8701,\n",
      "         -0.9053,  0.4367, -0.7327,  0.4686, -1.2452, -1.5630, -1.6064,  0.2162,\n",
      "          1.4465,  0.4889, -0.6909,  0.4870],\n",
      "        [-1.4218,  0.1867, -1.6763,  0.9469, -0.2091,  0.5207, -1.5274,  0.6969,\n",
      "         -0.0989,  1.2544,  0.5575, -0.9918, -0.4043,  1.2631,  1.0366, -0.6784,\n",
      "         -0.1153,  0.0199,  0.8133, -1.8205,  0.1534,  0.8680, -0.7286,  1.4453,\n",
      "          0.5078, -0.2035, -0.0176, -1.2107,  1.6066, -0.3830, -0.1607, -0.9890,\n",
      "         -0.5227, -0.5964, -1.8855, -0.7930, -0.1720,  1.3546,  0.4985, -0.4498,\n",
      "         -0.9277, -1.2261,  0.8756, -0.3121,  0.5210, -0.4837, -0.0231,  1.5019,\n",
      "         -0.6073, -1.0526, -0.6669, -1.5485, -0.5088,  1.2786,  0.2757, -0.7405,\n",
      "          2.1886,  0.4890, -0.0970, -0.6282,  1.9290,  1.2367,  0.2787,  0.6450,\n",
      "          0.1064, -0.5384, -0.6078,  0.3691, -0.1042,  0.0698,  1.7527,  1.2114,\n",
      "          1.8351,  0.1360, -0.1133,  0.3505, -0.5757, -0.3303,  1.1072,  1.1346,\n",
      "         -0.0436, -1.6191, -0.0552, -0.5968,  0.7796, -0.4858, -0.8220, -1.7088,\n",
      "          0.8542,  0.7621, -0.8701, -0.4640,  1.2462,  1.0525,  0.0083, -0.5547,\n",
      "          2.1130,  1.3431, -0.6692, -1.4572],\n",
      "        [-0.9692, -0.1138,  0.2101, -0.2291, -1.2237,  0.8012, -1.9087,  0.7066,\n",
      "         -1.5035,  1.9132, -0.1200, -0.4179, -0.7158,  1.1303,  0.5179,  0.4278,\n",
      "          1.4598, -0.2339, -0.0608,  1.8629,  0.0668, -0.5798,  0.0255, -0.7610,\n",
      "          0.1727,  0.7565, -2.7708,  1.0995,  1.4521, -0.3968,  2.0808, -0.5435,\n",
      "          2.0151,  1.4222,  0.5683,  0.2128,  0.9318, -0.8420, -0.2976,  1.0433,\n",
      "          0.8516,  0.2074,  0.2612,  0.1071,  0.8712,  0.3370, -0.0945,  0.7210,\n",
      "         -0.6258, -1.3921,  0.2629, -0.9624, -1.0701, -1.8299,  0.5100, -1.1768,\n",
      "         -1.3222, -0.6204, -0.1167,  0.0833,  0.0809,  1.8958,  0.8436, -0.0740,\n",
      "         -1.3548,  0.3014,  0.8853, -0.6784,  0.6553,  0.9207, -1.0591, -0.6129,\n",
      "          1.5279,  0.9292, -0.5339, -1.5362,  1.8026, -0.1254, -1.8357,  1.2016,\n",
      "          1.0843, -1.2735, -1.6556, -0.7632,  0.3343, -1.9892,  1.1458,  0.1643,\n",
      "          0.3271,  0.1973,  1.8158, -0.1644, -0.3829, -0.7719, -1.3229,  0.4867,\n",
      "          0.0975, -0.7579,  1.6219,  0.5612],\n",
      "        [ 0.1558, -0.0810,  1.1736, -0.7313, -1.1481, -0.6059, -0.1522, -0.5253,\n",
      "          0.7432,  0.6876,  0.8736, -0.4406, -0.8473, -1.6311, -0.9311,  0.3638,\n",
      "          0.4238,  0.6803,  0.3233,  0.4485, -1.0132,  0.4519,  0.5397,  0.4993,\n",
      "          1.4991, -0.2468,  0.1658,  0.5713, -1.1064,  1.8128, -2.1473, -1.7022,\n",
      "         -0.7051,  1.3890,  1.3642, -0.5411, -0.8894,  0.6865, -2.5219,  0.3693,\n",
      "          0.6839, -0.3216,  0.4011,  0.7221,  1.0311,  1.1013, -0.1642, -1.7857,\n",
      "          0.4805,  1.6331, -0.0735, -0.1407,  1.4699, -1.2276, -1.8706, -2.2710,\n",
      "          0.7152,  0.2796,  0.5609, -0.7293, -0.6194,  0.1224,  0.4699,  1.4511,\n",
      "         -1.7733, -0.0464, -1.3428,  1.7978, -0.1725, -0.5754, -1.6280, -1.4692,\n",
      "          0.3023,  1.9752, -0.1295,  1.3550,  0.9043,  0.6758,  1.0474, -0.6708,\n",
      "          1.2367,  0.8889,  1.1222, -0.5317,  0.2492,  0.2337,  1.6508, -1.8324,\n",
      "         -1.1166, -0.2103,  0.4463, -0.0202,  0.0101, -2.3933,  0.4469,  1.1302,\n",
      "          0.6164, -0.6043, -0.3585, -1.5851]])\n",
      "Mat_mul: tensor([[ 2.0108e+01,  6.8184e+00, -1.2627e+01,  5.8416e+00, -2.1017e+01,\n",
      "          5.7953e+00, -1.3200e+01,  1.8442e+00,  3.0376e+00,  1.5934e+01,\n",
      "         -1.2509e+01,  1.5392e+01, -3.3270e+00,  8.4611e+00,  5.7734e+00,\n",
      "         -1.1856e+01,  4.4979e+00, -1.1565e+01, -3.2298e-01, -2.0804e+01,\n",
      "         -3.4213e+00, -1.1482e+01,  6.5309e+00, -8.5748e+00, -5.5857e+00,\n",
      "         -1.7199e+00,  2.3785e+00, -4.9505e+00,  1.0179e+01, -1.0127e+01,\n",
      "         -5.5932e+00, -2.1247e+01,  1.8589e+01,  1.1850e+01, -1.8848e+01,\n",
      "         -4.8065e+00,  6.8135e+00,  2.1841e+00,  8.5455e+00, -6.8798e+00,\n",
      "          1.4690e+01,  8.3320e+00, -4.9629e+00, -9.3329e-01,  1.4376e+01,\n",
      "          9.0663e+00, -7.2270e+00, -4.7241e+00, -1.9635e+01, -9.9841e+00,\n",
      "          6.4952e-01, -4.4841e+00,  6.9929e+00,  8.9597e+00,  7.9532e+00,\n",
      "         -7.2360e+00, -6.5032e+00,  6.3962e+00,  1.1214e+01, -5.5354e+00,\n",
      "         -7.9486e+00, -1.2113e+00, -7.0038e+00,  9.8352e+00, -1.3993e+01,\n",
      "         -8.2088e-03, -2.0504e+01, -3.0659e+00, -7.3243e+00, -5.6366e+00,\n",
      "          8.1382e+00,  1.4279e+01,  2.9945e-01, -6.4665e+00,  2.0080e+01,\n",
      "         -5.4044e+00, -9.3986e+00,  6.8551e+00,  3.6290e+00,  1.2438e-01,\n",
      "          1.4657e+01, -9.1338e+00, -4.9156e+00,  1.0562e+01,  8.1365e+00,\n",
      "          4.4640e+00,  6.2001e+00, -4.0455e+00, -7.4836e+00, -6.2318e+00,\n",
      "         -7.6925e+00,  7.6033e+00, -6.8856e+00, -3.5648e+00, -1.7155e+01,\n",
      "          7.5379e+00,  5.0877e+00,  1.3626e+01,  7.3764e+00,  2.2415e+00],\n",
      "        [-2.4873e+00, -3.9128e-01,  1.4374e+00, -1.1986e+00, -4.2252e+00,\n",
      "          1.7267e+00, -5.2656e+00,  1.4800e+00, -3.4676e+00,  5.7540e+00,\n",
      "          3.5048e-01, -1.4893e+00, -2.5679e+00,  1.8869e+00,  6.6918e-01,\n",
      "          1.3930e+00,  4.3247e+00, -3.9430e-02,  1.7139e-01,  5.4391e+00,\n",
      "         -5.3915e-01, -1.2569e+00,  5.8932e-01, -1.7325e+00,  1.7172e+00,\n",
      "          1.8467e+00, -7.4135e+00,  3.3931e+00,  3.1314e+00,  2.1538e-01,\n",
      "          4.0186e+00, -2.8342e+00,  4.9494e+00,  4.9588e+00,  2.5088e+00,\n",
      "          5.3047e-02,  1.9429e+00, -1.6890e+00, -2.7855e+00,  3.1237e+00,\n",
      "          2.8964e+00,  2.0744e-01,  9.6488e-01,  9.1938e-01,  3.2170e+00,\n",
      "          1.8514e+00, -3.8490e-01,  5.5943e-01, -1.3611e+00, -2.6083e+00,\n",
      "          6.4131e-01, -2.7155e+00, -1.7403e+00, -5.9532e+00,  5.1703e-03,\n",
      "         -4.9795e+00, -3.0433e+00, -1.4907e+00,  1.5706e-01, -3.9553e-01,\n",
      "         -2.8354e-01,  5.3286e+00,  2.7270e+00,  9.0147e-01, -5.0832e+00,\n",
      "          7.2588e-01,  1.3725e+00, -5.2831e-01,  1.5244e+00,  2.1025e+00,\n",
      "         -4.1190e+00, -2.8302e+00,  4.3882e+00,  4.0403e+00, -1.5035e+00,\n",
      "         -3.1345e+00,  5.6177e+00,  2.5026e-01, -4.2220e+00,  2.6869e+00,\n",
      "          3.9098e+00, -2.7833e+00, -3.6932e+00, -2.4743e+00,  1.0633e+00,\n",
      "         -5.2464e+00,  4.4443e+00, -9.3798e-01, -2.9321e-02,  3.9557e-01,\n",
      "          5.2506e+00, -4.3844e-01, -1.0992e+00, -4.0508e+00, -3.3347e+00,\n",
      "          2.2180e+00,  8.2231e-01, -2.5084e+00,  4.0970e+00,  3.1471e-01],\n",
      "        [-1.2991e+01, -6.1318e-01, -6.5806e+00,  7.1227e+00, -4.0156e+00,\n",
      "          9.0723e+00, -3.0955e+00,  6.4080e-01,  1.2659e+00,  9.5785e+00,\n",
      "          8.6070e-02, -4.4713e+00, -8.9303e-01,  1.7578e+01,  9.2190e+00,\n",
      "         -8.2735e+00,  5.2318e+00,  6.9515e-01,  8.7332e+00, -3.4232e+00,\n",
      "          1.1644e+01, -4.3167e+00,  4.8598e+00, -3.5567e-01, -1.4020e+00,\n",
      "         -3.0997e+00, -2.1200e+00, -7.3166e+00,  1.7073e+01, -7.7341e+00,\n",
      "         -2.7007e+00, -1.0600e+01, -1.8858e+00, -7.5863e+00, -1.7250e+01,\n",
      "         -1.2477e+01,  5.3474e+00,  1.3870e+01, -6.4529e+00,  1.4955e+00,\n",
      "         -1.4457e+01, -1.4765e+01,  8.5868e+00, -1.1146e+00,  3.0864e+00,\n",
      "         -4.3724e-01,  1.9384e+00,  8.2265e+00, -1.4129e+01, -1.1989e+01,\n",
      "         -3.6214e+00, -1.0499e+01, -3.5924e+00,  6.1761e+00,  1.2326e+01,\n",
      "         -1.5617e+00,  9.7320e+00,  1.4536e+00,  4.6754e+00, -6.9618e-01,\n",
      "          1.7950e+01,  1.0696e+01,  7.8964e+00, -3.8731e+00,  9.7161e-01,\n",
      "         -1.3276e+01, -8.7466e+00, -4.3101e+00, -1.3818e+01,  1.9330e+00,\n",
      "          9.6669e+00,  3.5952e+00,  1.2621e+01, -9.3383e-01,  1.2836e+00,\n",
      "         -2.4217e+00,  3.3534e+00, -6.6817e+00,  5.5585e+00,  9.8920e+00,\n",
      "         -2.7548e+00, -1.6196e+01, -3.0218e+00, -1.4116e-01, -6.2572e-01,\n",
      "         -9.4749e+00, -3.9343e+00, -9.1052e+00,  3.7419e+00,  1.0631e+01,\n",
      "         -4.1306e+00, -7.7213e+00,  9.6683e+00, -4.4573e+00, -1.1234e+01,\n",
      "         -4.5183e+00,  4.5658e+00,  1.0810e+01, -1.0347e+00, -8.7917e+00],\n",
      "        [ 6.5731e-01,  1.9705e+00,  7.0679e+00, -5.9872e+00, -7.3465e+00,\n",
      "          4.9143e+00,  4.0116e-02,  4.2265e+00,  6.2560e+00,  6.9874e+00,\n",
      "         -7.0767e-01, -6.2907e+00, -9.8013e+00, -9.9476e+00, -4.9584e+00,\n",
      "          3.4400e-01,  3.3209e+00,  2.6883e+00,  5.2270e+00,  4.9631e+00,\n",
      "         -5.7139e+00,  1.0791e+00,  1.2042e+00,  3.4785e+00,  7.4671e+00,\n",
      "          4.4911e+00, -4.7102e+00,  2.9969e+00, -1.0910e+00,  4.6841e+00,\n",
      "         -7.5585e+00, -1.0237e+00, -9.0841e-03,  9.0799e+00,  3.7453e-01,\n",
      "          2.1412e+00,  1.9846e+00,  1.9372e+00, -8.5251e+00,  1.0906e+01,\n",
      "         -1.0240e+00,  2.6249e+00,  4.4682e+00, -5.5499e+00,  1.6497e+00,\n",
      "          3.9987e+00,  2.5605e+00, -4.6078e+00,  1.1814e+01, -6.6078e-01,\n",
      "          8.4268e+00,  3.4590e+00,  5.1971e+00, -5.7924e+00, -3.3002e+00,\n",
      "         -1.1787e+01, -4.4017e+00,  2.0324e+00,  3.7890e+00, -4.6791e+00,\n",
      "          1.0898e+00,  9.3908e+00, -9.5395e-01,  3.1210e+00, -5.1684e+00,\n",
      "         -9.8154e+00, -8.0472e+00,  2.9718e+00, -2.5107e+00, -4.6335e+00,\n",
      "         -7.5953e+00, -3.3807e-01,  6.4516e+00,  1.0494e+01,  2.3189e+00,\n",
      "         -3.8983e-01,  7.0009e-01,  1.4596e+00,  6.6818e-01,  2.2931e+00,\n",
      "          7.4645e+00, -1.6074e+00,  2.5780e+00, -1.2639e+00, -2.5760e+00,\n",
      "         -4.5231e+00,  8.1185e+00, -1.0051e+01, -8.2818e+00, -5.3177e+00,\n",
      "          1.9383e+00,  3.0798e+00, -1.3684e+00, -6.3726e+00, -9.5922e-01,\n",
      "          1.9774e+00,  5.6824e-03,  6.3572e-01,  2.1223e+00, -1.0687e+01],\n",
      "        [ 2.5825e+00,  3.5188e+00, -1.5711e+01,  8.6632e+00, -9.2897e+00,\n",
      "          3.6104e+00, -1.6206e+01,  6.6003e+00, -3.9753e+00,  1.4843e+01,\n",
      "         -6.5550e+00,  4.2039e+00, -6.1933e+00,  9.2113e+00,  7.5499e+00,\n",
      "         -8.7144e+00, -1.0581e+00, -6.6956e+00,  5.3161e+00, -2.4175e+01,\n",
      "         -5.4647e+00, -1.1307e-01, -6.2833e+00,  5.4156e+00, -4.8496e+00,\n",
      "         -4.4038e+00,  4.1271e+00, -6.6552e+00,  1.4150e+01, -5.6415e+00,\n",
      "         -7.7030e+00, -1.8157e+01,  8.4817e+00, -1.1494e+00, -1.6148e+01,\n",
      "         -4.9972e+00, -2.4184e+00,  1.0782e+01,  8.8089e+00, -9.8714e+00,\n",
      "          1.3295e+00,  3.4039e-01,  4.2888e+00, -2.1765e+00,  1.3964e+01,\n",
      "          6.4172e-01, -4.2193e+00,  3.3724e+00, -1.8378e+01, -7.5334e+00,\n",
      "         -7.2752e+00, -1.5833e+01, -3.9787e-01,  1.1293e+01,  1.7512e+00,\n",
      "         -1.1191e+01,  1.1310e+01,  7.2972e+00,  9.1726e+00, -4.8078e+00,\n",
      "          7.1037e+00,  5.8974e-01, -3.4905e+00,  1.4074e+01, -8.0352e+00,\n",
      "          3.2119e-01, -1.3814e+01,  2.7461e+00,  6.4636e-01, -1.2295e+00,\n",
      "          1.5401e+01,  1.4106e+01,  9.9323e+00,  9.7371e-01,  7.6570e+00,\n",
      "         -4.4191e-01, -1.0612e+01,  3.3063e+00,  8.9887e+00,  8.2939e+00,\n",
      "          4.9268e+00, -1.1933e+01, -1.2745e+00, -2.1749e+00,  1.2592e+01,\n",
      "          1.4348e+00, -3.0121e+00, -9.9045e+00,  3.0850e+00, -3.1087e-01,\n",
      "         -5.6342e+00, -9.1641e-01,  5.6828e+00,  4.4431e+00, -5.8935e+00,\n",
      "         -3.5086e-02,  1.4269e+01,  1.4196e+01, -2.1360e+00, -7.8795e+00],\n",
      "        [ 1.0082e+01,  1.0309e+01,  1.2251e+01, -1.0760e+01, -3.7854e+01,\n",
      "          2.0377e+01, -3.6975e+01,  1.3453e+01, -4.7381e+00,  5.1125e+01,\n",
      "         -9.3790e+00,  6.8979e-01, -3.0905e+01, -9.4858e+00,  1.4240e+00,\n",
      "         -4.8619e-01,  2.0525e+01, -8.2260e+00, -1.4971e+00,  4.8096e+00,\n",
      "         -2.5203e+01, -1.3999e+01,  8.4888e+00, -1.2260e+00,  1.0010e+01,\n",
      "          1.0792e+01, -1.9857e+01,  1.7710e+01,  1.2776e+01,  1.5301e+01,\n",
      "         -3.1768e+00, -3.8998e+01,  3.1641e+01,  4.1737e+01,  4.6888e+00,\n",
      "          7.2395e+00,  7.2385e+00,  5.1124e-02, -2.0525e+01,  1.4501e+01,\n",
      "          1.8259e+01,  2.0785e+01,  9.1548e+00, -6.9612e+00,  2.4216e+01,\n",
      "          1.6999e+01, -4.7289e+00, -1.3989e+01, -9.1059e+00, -8.3918e+00,\n",
      "          1.6507e+01, -4.8275e+00,  7.6180e+00, -2.2594e+01, -9.9663e+00,\n",
      "         -4.3192e+01, -2.0023e+01,  1.0112e+01,  1.1970e+01, -1.3357e+01,\n",
      "         -6.9346e+00,  1.4692e+01, -1.9787e+00,  2.0850e+01, -3.6645e+01,\n",
      "         -1.5163e+00, -2.5589e+01,  8.8788e+00,  2.6390e+00, -6.2602e+00,\n",
      "         -2.5925e+01, -1.9881e+00,  2.0820e+01,  2.6709e+01,  1.2383e+01,\n",
      "         -8.0996e+00,  1.2580e+01,  3.2845e+00, -8.7823e+00,  1.6720e+01,\n",
      "          3.7315e+01, -1.1508e+01, -7.6544e+00, -8.6631e+00,  1.0789e+01,\n",
      "         -1.4320e+01,  3.6010e+01, -2.5113e+01, -1.9128e+01, -1.4684e+01,\n",
      "          1.8675e+01,  1.7314e+01,  2.5170e+00, -3.3522e+01, -1.2106e+01,\n",
      "          2.3710e+01, -2.4127e+00,  2.0261e+00,  2.6692e+01, -1.6749e+01],\n",
      "        [ 2.1317e+01,  1.2641e+01, -1.4324e+01, -2.5900e+00, -2.9590e+01,\n",
      "          8.1364e+00, -1.8112e+01,  3.5601e+00,  1.0006e+01,  2.5854e+01,\n",
      "         -7.2498e+00,  2.0090e+01, -3.0331e+00,  7.2491e+00,  1.7403e+01,\n",
      "         -1.7886e+01,  8.7812e+00, -2.3015e+01, -8.9270e+00, -2.8786e+01,\n",
      "         -8.4775e+00, -1.9618e+01,  3.3959e+00, -7.8848e+00, -1.8536e+01,\n",
      "          1.5663e+00,  9.1638e-01, -8.4802e+00,  6.4416e+00,  5.5945e+00,\n",
      "         -7.2140e+00, -1.6060e+01,  2.1542e+01,  1.9063e+01, -1.1960e+01,\n",
      "          3.0280e+00, -9.6233e+00, -6.2252e+00,  6.7750e-01, -5.1361e+00,\n",
      "          1.3561e+01,  2.8091e+01, -1.0802e+00, -1.6411e+01,  1.8306e+01,\n",
      "         -1.0017e+01, -5.5848e+00,  7.4752e+00, -1.4285e+01, -3.7050e+00,\n",
      "         -1.5873e+00, -1.1062e+01,  2.8318e+00,  1.8061e+01,  1.7429e+00,\n",
      "          1.2532e+00, -1.2677e+01,  2.0075e+01,  9.3579e+00,  2.2875e+00,\n",
      "         -2.9957e+00, -1.4255e+01, -2.8251e+00,  9.2019e+00, -1.7891e+01,\n",
      "          4.2207e+00, -3.3453e+01,  8.4432e+00, -1.1063e+00, -4.8888e+00,\n",
      "          1.6506e+01,  2.1093e+01,  1.1645e+01, -1.1153e+01,  3.0146e+01,\n",
      "         -3.6283e+00, -1.2636e+01, -4.4038e+00,  1.3897e+01,  1.2240e+01,\n",
      "          2.5101e+01, -1.3126e+01, -6.7956e+00,  2.4417e+01,  1.9750e+01,\n",
      "          6.8743e+00,  9.6895e+00, -1.6734e+01, -2.3252e+00, -8.5260e+00,\n",
      "         -3.7587e+00,  5.1674e+00, -1.0367e+01,  1.0633e+01, -1.3944e+01,\n",
      "          1.2199e+01,  2.2768e+00,  6.2508e+00,  1.1484e+01,  7.0400e+00],\n",
      "        [-6.5546e-01,  3.1861e+00, -5.2395e+00, -9.4236e+00, -1.0442e+01,\n",
      "          3.2466e-02,  2.0738e+00, -9.2153e-01,  1.2203e+01,  5.4103e+00,\n",
      "          8.3568e+00,  1.7651e+00,  4.8038e+00,  3.7666e+00,  1.0444e+01,\n",
      "         -9.0705e+00,  6.8347e+00, -7.2698e+00, -1.7758e+00, -3.5425e+00,\n",
      "          3.0556e+00, -7.3728e+00,  7.4476e-02, -3.1347e+00, -8.7812e+00,\n",
      "          2.9841e+00, -7.0570e+00, -7.8099e+00, -2.8992e+00,  8.2826e+00,\n",
      "         -1.4733e+00,  1.1318e+01,  7.3046e-01,  6.9298e+00,  2.2464e+00,\n",
      "          7.1650e-01, -1.3070e+01, -7.5167e+00, -9.8229e+00,  5.8470e+00,\n",
      "          5.2497e-01,  1.1939e+01,  1.2620e+00, -1.3270e+01,  5.6529e+00,\n",
      "         -1.7856e+01,  3.0510e+00,  1.5271e+01,  1.0299e+01,  2.0361e+00,\n",
      "         -4.7482e+00, -8.4778e+00, -3.3877e+00,  8.4228e+00, -1.1078e+00,\n",
      "          1.0336e+01, -6.1361e+00,  1.0465e+01, -2.0859e-01,  7.5495e+00,\n",
      "          5.6715e+00, -3.8874e+00,  1.0832e+01, -4.3380e+00, -4.6776e+00,\n",
      "         -2.1533e+00, -1.1971e+01,  7.8157e+00, -3.0867e-01,  3.8269e+00,\n",
      "          1.2340e+01,  5.5583e+00,  1.4621e+01, -5.6480e+00,  1.1668e+01,\n",
      "          4.1203e-02, -1.2420e+00, -7.9295e+00,  1.0239e+01,  8.3723e+00,\n",
      "          1.0253e+01, -6.8063e+00, -5.2748e+00,  1.8499e+01,  8.9247e+00,\n",
      "         -1.1835e+00,  3.7574e+00, -1.1909e+01,  4.8403e+00,  9.1977e-01,\n",
      "          2.0041e+00, -7.5403e+00, -1.2809e+01,  1.5423e+01, -4.4278e+00,\n",
      "          2.1746e+00,  4.7341e+00, -9.1874e+00, -1.3832e-01,  7.0847e+00]])\n",
      "After mat mul: torch.Size([8, 100])\n",
      "Bias shape: torch.Size([1, 100])\n",
      "Bias: tensor([[ 0.4258, -1.0735, -0.4963,  1.6631,  0.0926, -0.5007,  0.2809, -0.2413,\n",
      "         -0.1767,  0.0700,  1.0809,  1.5049, -0.2328,  0.8872,  0.8131,  1.1063,\n",
      "         -1.4301,  0.2844,  0.1823, -1.9053,  1.4060,  0.5567, -0.8469, -1.2564,\n",
      "         -1.2962, -0.6060,  2.0296,  0.4973,  0.9965, -0.6968,  0.1826, -1.2874,\n",
      "         -1.1499,  1.1326, -0.2890,  0.2950, -0.4171,  0.2918, -1.1863,  0.5398,\n",
      "         -0.3843, -1.1302,  1.2678,  1.0875, -1.0604, -2.2901,  0.7306,  0.9418,\n",
      "          1.0275,  0.6726, -0.5642, -0.2420, -0.2622, -1.6228, -1.8783, -0.5563,\n",
      "         -0.5640, -0.8891,  1.3323,  1.1682, -0.1545, -0.0724, -0.9550,  0.9977,\n",
      "          0.6228, -1.2400,  1.3316, -0.6494,  0.1916,  0.3353,  0.5360,  1.1759,\n",
      "          0.1023, -0.8032, -0.5184, -0.9189,  0.2152, -0.5854, -2.9381, -1.2093,\n",
      "          1.2267,  1.6279, -0.0840,  0.3677,  0.4241, -1.2494, -1.3398,  0.1604,\n",
      "          0.8923,  0.2470, -2.8235, -1.6970, -0.2070,  0.6402,  1.5081,  1.0933,\n",
      "         -0.3311,  0.8979, -0.3822,  0.9082]])\n",
      "Mat mul plus bias: tensor([[ 2.0534e+01,  5.7450e+00, -1.3123e+01,  7.5047e+00, -2.0924e+01,\n",
      "          5.2946e+00, -1.2919e+01,  1.6029e+00,  2.8609e+00,  1.6004e+01,\n",
      "         -1.1428e+01,  1.6897e+01, -3.5598e+00,  9.3483e+00,  6.5865e+00,\n",
      "         -1.0750e+01,  3.0678e+00, -1.1281e+01, -1.4072e-01, -2.2709e+01,\n",
      "         -2.0153e+00, -1.0925e+01,  5.6840e+00, -9.8311e+00, -6.8819e+00,\n",
      "         -2.3259e+00,  4.4081e+00, -4.4532e+00,  1.1176e+01, -1.0824e+01,\n",
      "         -5.4107e+00, -2.2535e+01,  1.7439e+01,  1.2983e+01, -1.9137e+01,\n",
      "         -4.5115e+00,  6.3964e+00,  2.4759e+00,  7.3591e+00, -6.3400e+00,\n",
      "          1.4306e+01,  7.2018e+00, -3.6951e+00,  1.5418e-01,  1.3316e+01,\n",
      "          6.7762e+00, -6.4964e+00, -3.7823e+00, -1.8608e+01, -9.3115e+00,\n",
      "          8.5292e-02, -4.7261e+00,  6.7308e+00,  7.3369e+00,  6.0749e+00,\n",
      "         -7.7923e+00, -7.0672e+00,  5.5071e+00,  1.2546e+01, -4.3672e+00,\n",
      "         -8.1031e+00, -1.2837e+00, -7.9588e+00,  1.0833e+01, -1.3370e+01,\n",
      "         -1.2482e+00, -1.9173e+01, -3.7153e+00, -7.1326e+00, -5.3013e+00,\n",
      "          8.6742e+00,  1.5455e+01,  4.0180e-01, -7.2697e+00,  1.9562e+01,\n",
      "         -6.3233e+00, -9.1835e+00,  6.2697e+00,  6.9085e-01, -1.0849e+00,\n",
      "          1.5884e+01, -7.5059e+00, -4.9997e+00,  1.0930e+01,  8.5606e+00,\n",
      "          3.2145e+00,  4.8603e+00, -3.8851e+00, -6.5914e+00, -5.9848e+00,\n",
      "         -1.0516e+01,  5.9063e+00, -7.0927e+00, -2.9246e+00, -1.5647e+01,\n",
      "          8.6312e+00,  4.7565e+00,  1.4524e+01,  6.9942e+00,  3.1497e+00],\n",
      "        [-2.0615e+00, -1.4648e+00,  9.4110e-01,  4.6452e-01, -4.1326e+00,\n",
      "          1.2260e+00, -4.9847e+00,  1.2387e+00, -3.6443e+00,  5.8240e+00,\n",
      "          1.4314e+00,  1.5665e-02, -2.8007e+00,  2.7741e+00,  1.4823e+00,\n",
      "          2.4993e+00,  2.8947e+00,  2.4497e-01,  3.5366e-01,  3.5338e+00,\n",
      "          8.6683e-01, -7.0017e-01, -2.5754e-01, -2.9889e+00,  4.2097e-01,\n",
      "          1.2407e+00, -5.3839e+00,  3.8904e+00,  4.1279e+00, -4.8147e-01,\n",
      "          4.2011e+00, -4.1216e+00,  3.7996e+00,  6.0914e+00,  2.2199e+00,\n",
      "          3.4800e-01,  1.5258e+00, -1.3971e+00, -3.9718e+00,  3.6636e+00,\n",
      "          2.5121e+00, -9.2274e-01,  2.2327e+00,  2.0069e+00,  2.1567e+00,\n",
      "         -4.3873e-01,  3.4567e-01,  1.5012e+00, -3.3353e-01, -1.9357e+00,\n",
      "          7.7084e-02, -2.9576e+00, -2.0024e+00, -7.5760e+00, -1.8731e+00,\n",
      "         -5.5359e+00, -3.6073e+00, -2.3798e+00,  1.4894e+00,  7.7262e-01,\n",
      "         -4.3805e-01,  5.2563e+00,  1.7720e+00,  1.8992e+00, -4.4604e+00,\n",
      "         -5.1411e-01,  2.7041e+00, -1.1777e+00,  1.7160e+00,  2.4378e+00,\n",
      "         -3.5830e+00, -1.6543e+00,  4.4906e+00,  3.2371e+00, -2.0219e+00,\n",
      "         -4.0534e+00,  5.8329e+00, -3.3518e-01, -7.1601e+00,  1.4776e+00,\n",
      "          5.1365e+00, -1.1554e+00, -3.7772e+00, -2.1067e+00,  1.4874e+00,\n",
      "         -6.4958e+00,  3.1045e+00, -7.7759e-01,  8.6296e-01,  6.4253e-01,\n",
      "          2.4271e+00, -2.1354e+00, -1.3062e+00, -3.4106e+00, -1.8266e+00,\n",
      "          3.3113e+00,  4.9117e-01, -1.6105e+00,  3.7147e+00,  1.2229e+00],\n",
      "        [-1.2566e+01, -1.6867e+00, -7.0768e+00,  8.7858e+00, -3.9230e+00,\n",
      "          8.5716e+00, -2.8146e+00,  3.9947e-01,  1.0892e+00,  9.6485e+00,\n",
      "          1.1670e+00, -2.9663e+00, -1.1258e+00,  1.8465e+01,  1.0032e+01,\n",
      "         -7.1672e+00,  3.8018e+00,  9.7955e-01,  8.9155e+00, -5.3285e+00,\n",
      "          1.3050e+01, -3.7600e+00,  4.0129e+00, -1.6120e+00, -2.6983e+00,\n",
      "         -3.7057e+00, -9.0458e-02, -6.8192e+00,  1.8070e+01, -8.4309e+00,\n",
      "         -2.5181e+00, -1.1887e+01, -3.0356e+00, -6.4537e+00, -1.7539e+01,\n",
      "         -1.2182e+01,  4.9304e+00,  1.4162e+01, -7.6392e+00,  2.0354e+00,\n",
      "         -1.4842e+01, -1.5895e+01,  9.8546e+00, -2.7154e-02,  2.0261e+00,\n",
      "         -2.7273e+00,  2.6689e+00,  9.1683e+00, -1.3101e+01, -1.1316e+01,\n",
      "         -4.1856e+00, -1.0741e+01, -3.8546e+00,  4.5533e+00,  1.0448e+01,\n",
      "         -2.1181e+00,  9.1680e+00,  5.6450e-01,  6.0077e+00,  4.7197e-01,\n",
      "          1.7796e+01,  1.0623e+01,  6.9414e+00, -2.8754e+00,  1.5944e+00,\n",
      "         -1.4516e+01, -7.4150e+00, -4.9595e+00, -1.3626e+01,  2.2683e+00,\n",
      "          1.0203e+01,  4.7711e+00,  1.2723e+01, -1.7370e+00,  7.6521e-01,\n",
      "         -3.3406e+00,  3.5686e+00, -7.2671e+00,  2.6204e+00,  8.6827e+00,\n",
      "         -1.5281e+00, -1.4568e+01, -3.1058e+00,  2.2651e-01, -2.0161e-01,\n",
      "         -1.0724e+01, -5.2740e+00, -8.9448e+00,  4.6342e+00,  1.0878e+01,\n",
      "         -6.9541e+00, -9.4183e+00,  9.4613e+00, -3.8171e+00, -9.7261e+00,\n",
      "         -3.4250e+00,  4.2346e+00,  1.1708e+01, -1.4169e+00, -7.8835e+00],\n",
      "        [ 1.0831e+00,  8.9698e-01,  6.5717e+00, -4.3241e+00, -7.2539e+00,\n",
      "          4.4136e+00,  3.2100e-01,  3.9852e+00,  6.0793e+00,  7.0574e+00,\n",
      "          3.7327e-01, -4.7858e+00, -1.0034e+01, -9.0604e+00, -4.1452e+00,\n",
      "          1.4503e+00,  1.8908e+00,  2.9727e+00,  5.4093e+00,  3.0578e+00,\n",
      "         -4.3079e+00,  1.6358e+00,  3.5737e-01,  2.2221e+00,  6.1709e+00,\n",
      "          3.8852e+00, -2.6806e+00,  3.4943e+00, -9.4488e-02,  3.9872e+00,\n",
      "         -7.3759e+00, -2.3111e+00, -1.1589e+00,  1.0213e+01,  8.5574e-02,\n",
      "          2.4362e+00,  1.5676e+00,  2.2290e+00, -9.7115e+00,  1.1446e+01,\n",
      "         -1.4083e+00,  1.4947e+00,  5.7361e+00, -4.4624e+00,  5.8935e-01,\n",
      "          1.7086e+00,  3.2911e+00, -3.6660e+00,  1.2842e+01,  1.1785e-02,\n",
      "          7.8626e+00,  3.2170e+00,  4.9349e+00, -7.4152e+00, -5.1785e+00,\n",
      "         -1.2343e+01, -4.9658e+00,  1.1434e+00,  5.1213e+00, -3.5109e+00,\n",
      "          9.3529e-01,  9.3185e+00, -1.9089e+00,  4.1187e+00, -4.5456e+00,\n",
      "         -1.1055e+01, -6.7155e+00,  2.3224e+00, -2.3191e+00, -4.2982e+00,\n",
      "         -7.0592e+00,  8.3781e-01,  6.5539e+00,  9.6905e+00,  1.8005e+00,\n",
      "         -1.3087e+00,  9.1528e-01,  8.7412e-01, -2.2699e+00,  1.0838e+00,\n",
      "          8.6912e+00,  2.0536e-02,  2.4940e+00, -8.9627e-01, -2.1519e+00,\n",
      "         -5.7726e+00,  6.7787e+00, -9.8906e+00, -7.3896e+00, -5.0708e+00,\n",
      "         -8.8523e-01,  1.3828e+00, -1.5755e+00, -5.7324e+00,  5.4891e-01,\n",
      "          3.0707e+00, -3.2546e-01,  1.5336e+00,  1.7401e+00, -9.7789e+00],\n",
      "        [ 3.0083e+00,  2.4453e+00, -1.6208e+01,  1.0326e+01, -9.1971e+00,\n",
      "          3.1097e+00, -1.5925e+01,  6.3590e+00, -4.1519e+00,  1.4913e+01,\n",
      "         -5.4740e+00,  5.7088e+00, -6.4261e+00,  1.0098e+01,  8.3631e+00,\n",
      "         -7.6081e+00, -2.4882e+00, -6.4112e+00,  5.4983e+00, -2.6080e+01,\n",
      "         -4.0587e+00,  4.4363e-01, -7.1301e+00,  4.1593e+00, -6.1458e+00,\n",
      "         -5.0098e+00,  6.1567e+00, -6.1579e+00,  1.5146e+01, -6.3383e+00,\n",
      "         -7.5204e+00, -1.9445e+01,  7.3319e+00, -1.6862e-02, -1.6437e+01,\n",
      "         -4.7022e+00, -2.8354e+00,  1.1073e+01,  7.6225e+00, -9.3316e+00,\n",
      "          9.4524e-01, -7.8978e-01,  5.5566e+00, -1.0890e+00,  1.2904e+01,\n",
      "         -1.6484e+00, -3.4887e+00,  4.3142e+00, -1.7350e+01, -6.8608e+00,\n",
      "         -7.8394e+00, -1.6075e+01, -6.6005e-01,  9.6699e+00, -1.2709e-01,\n",
      "         -1.1747e+01,  1.0746e+01,  6.4081e+00,  1.0505e+01, -3.6396e+00,\n",
      "          6.9492e+00,  5.1736e-01, -4.4455e+00,  1.5072e+01, -7.4124e+00,\n",
      "         -9.1880e-01, -1.2483e+01,  2.0968e+00,  8.3799e-01, -8.9416e-01,\n",
      "          1.5937e+01,  1.5282e+01,  1.0035e+01,  1.7054e-01,  7.1386e+00,\n",
      "         -1.3608e+00, -1.0396e+01,  2.7209e+00,  6.0506e+00,  7.0846e+00,\n",
      "          6.1534e+00, -1.0305e+01, -1.3585e+00, -1.8072e+00,  1.3016e+01,\n",
      "          1.8540e-01, -4.3518e+00, -9.7441e+00,  3.9773e+00, -6.3914e-02,\n",
      "         -8.4577e+00, -2.6134e+00,  5.4758e+00,  5.0833e+00, -4.3853e+00,\n",
      "          1.0582e+00,  1.3938e+01,  1.5094e+01, -2.5183e+00, -6.9713e+00],\n",
      "        [ 1.0508e+01,  9.2356e+00,  1.1754e+01, -9.0965e+00, -3.7762e+01,\n",
      "          1.9876e+01, -3.6694e+01,  1.3212e+01, -4.9147e+00,  5.1195e+01,\n",
      "         -8.2980e+00,  2.1947e+00, -3.1137e+01, -8.5986e+00,  2.2371e+00,\n",
      "          6.2011e-01,  1.9095e+01, -7.9416e+00, -1.3148e+00,  2.9043e+00,\n",
      "         -2.3797e+01, -1.3442e+01,  7.6419e+00, -2.4823e+00,  8.7138e+00,\n",
      "          1.0186e+01, -1.7828e+01,  1.8207e+01,  1.3772e+01,  1.4604e+01,\n",
      "         -2.9942e+00, -4.0285e+01,  3.0491e+01,  4.2869e+01,  4.3999e+00,\n",
      "          7.5345e+00,  6.8215e+00,  3.4295e-01, -2.1711e+01,  1.5040e+01,\n",
      "          1.7875e+01,  1.9655e+01,  1.0423e+01, -5.8737e+00,  2.3156e+01,\n",
      "          1.4709e+01, -3.9983e+00, -1.3047e+01, -8.0783e+00, -7.7192e+00,\n",
      "          1.5943e+01, -5.0695e+00,  7.3558e+00, -2.4217e+01, -1.1845e+01,\n",
      "         -4.3749e+01, -2.0587e+01,  9.2231e+00,  1.3302e+01, -1.2189e+01,\n",
      "         -7.0891e+00,  1.4620e+01, -2.9337e+00,  2.1848e+01, -3.6022e+01,\n",
      "         -2.7563e+00, -2.4257e+01,  8.2295e+00,  2.8306e+00, -5.9249e+00,\n",
      "         -2.5389e+01, -8.1223e-01,  2.0922e+01,  2.5906e+01,  1.1865e+01,\n",
      "         -9.0185e+00,  1.2796e+01,  2.6991e+00, -1.1720e+01,  1.5511e+01,\n",
      "          3.8542e+01, -9.8805e+00, -7.7385e+00, -8.2955e+00,  1.1213e+01,\n",
      "         -1.5570e+01,  3.4670e+01, -2.4953e+01, -1.8236e+01, -1.4437e+01,\n",
      "          1.5852e+01,  1.5617e+01,  2.3100e+00, -3.2882e+01, -1.0598e+01,\n",
      "          2.4803e+01, -2.7438e+00,  2.9240e+00,  2.6310e+01, -1.5841e+01],\n",
      "        [ 2.1743e+01,  1.1568e+01, -1.4820e+01, -9.2689e-01, -2.9498e+01,\n",
      "          7.6357e+00, -1.7831e+01,  3.3187e+00,  9.8296e+00,  2.5924e+01,\n",
      "         -6.1689e+00,  2.1595e+01, -3.2659e+00,  8.1363e+00,  1.8216e+01,\n",
      "         -1.6780e+01,  7.3511e+00, -2.2730e+01, -8.7448e+00, -3.0692e+01,\n",
      "         -7.0715e+00, -1.9061e+01,  2.5491e+00, -9.1411e+00, -1.9832e+01,\n",
      "          9.6031e-01,  2.9460e+00, -7.9829e+00,  7.4381e+00,  4.8976e+00,\n",
      "         -7.0315e+00, -1.7347e+01,  2.0392e+01,  2.0195e+01, -1.2249e+01,\n",
      "          3.3229e+00, -1.0040e+01, -5.9334e+00, -5.0885e-01, -4.5963e+00,\n",
      "          1.3177e+01,  2.6961e+01,  1.8766e-01, -1.5323e+01,  1.7245e+01,\n",
      "         -1.2307e+01, -4.8542e+00,  8.4170e+00, -1.3257e+01, -3.0324e+00,\n",
      "         -2.1515e+00, -1.1305e+01,  2.5696e+00,  1.6438e+01, -1.3535e-01,\n",
      "          6.9691e-01, -1.3241e+01,  1.9186e+01,  1.0690e+01,  3.4556e+00,\n",
      "         -3.1502e+00, -1.4327e+01, -3.7801e+00,  1.0200e+01, -1.7268e+01,\n",
      "          2.9807e+00, -3.2121e+01,  7.7939e+00, -9.1465e-01, -4.5535e+00,\n",
      "          1.7042e+01,  2.2269e+01,  1.1747e+01, -1.1956e+01,  2.9627e+01,\n",
      "         -4.5472e+00, -1.2421e+01, -4.9893e+00,  1.0959e+01,  1.1031e+01,\n",
      "          2.6327e+01, -1.1498e+01, -6.8796e+00,  2.4784e+01,  2.0174e+01,\n",
      "          5.6248e+00,  8.3498e+00, -1.6574e+01, -1.4329e+00, -8.2790e+00,\n",
      "         -6.5822e+00,  3.4704e+00, -1.0574e+01,  1.1273e+01, -1.2436e+01,\n",
      "          1.3292e+01,  1.9456e+00,  7.1487e+00,  1.1102e+01,  7.9482e+00],\n",
      "        [-2.2964e-01,  2.1126e+00, -5.7357e+00, -7.7606e+00, -1.0349e+01,\n",
      "         -4.6825e-01,  2.3547e+00, -1.1629e+00,  1.2026e+01,  5.4803e+00,\n",
      "          9.4377e+00,  3.2700e+00,  4.5710e+00,  4.6539e+00,  1.1257e+01,\n",
      "         -7.9642e+00,  5.4046e+00, -6.9854e+00, -1.5936e+00, -5.4478e+00,\n",
      "          4.4616e+00, -6.8161e+00, -7.7239e-01, -4.3911e+00, -1.0077e+01,\n",
      "          2.3781e+00, -5.0274e+00, -7.3125e+00, -1.9027e+00,  7.5857e+00,\n",
      "         -1.2908e+00,  1.0031e+01, -4.1940e-01,  8.0624e+00,  1.9574e+00,\n",
      "          1.0115e+00, -1.3487e+01, -7.2248e+00, -1.1009e+01,  6.3868e+00,\n",
      "          1.4072e-01,  1.0809e+01,  2.5299e+00, -1.2182e+01,  4.5925e+00,\n",
      "         -2.0146e+01,  3.7816e+00,  1.6213e+01,  1.1327e+01,  2.7087e+00,\n",
      "         -5.3124e+00, -8.7198e+00, -3.6499e+00,  6.7999e+00, -2.9861e+00,\n",
      "          9.7801e+00, -6.7001e+00,  9.5755e+00,  1.1238e+00,  8.7177e+00,\n",
      "          5.5170e+00, -3.9597e+00,  9.8775e+00, -3.3403e+00, -4.0548e+00,\n",
      "         -3.3933e+00, -1.0640e+01,  7.1663e+00, -1.1704e-01,  4.1622e+00,\n",
      "          1.2876e+01,  6.7342e+00,  1.4723e+01, -6.4512e+00,  1.1150e+01,\n",
      "         -8.7769e-01, -1.0268e+00, -8.5149e+00,  7.3012e+00,  7.1630e+00,\n",
      "          1.1480e+01, -5.1784e+00, -5.3588e+00,  1.8867e+01,  9.3488e+00,\n",
      "         -2.4330e+00,  2.4176e+00, -1.1748e+01,  5.7326e+00,  1.1667e+00,\n",
      "         -8.1941e-01, -9.2373e+00, -1.3016e+01,  1.6063e+01, -2.9197e+00,\n",
      "          3.2680e+00,  4.4030e+00, -8.2895e+00, -5.2056e-01,  7.9929e+00]])\n",
      "After adding bias: torch.Size([8, 100])\n",
      "Output shape: torch.Size([8, 100])\n",
      "Output: tensor([[2.0534e+01, 5.7450e+00, -0.0000e+00, 7.5047e+00, -0.0000e+00, 5.2946e+00,\n",
      "         -0.0000e+00, 1.6029e+00, 2.8609e+00, 1.6004e+01, -0.0000e+00, 1.6897e+01,\n",
      "         -0.0000e+00, 9.3483e+00, 6.5865e+00, -0.0000e+00, 3.0678e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.6840e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 4.4081e+00, -0.0000e+00, 1.1176e+01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 1.7439e+01, 1.2983e+01, -0.0000e+00, -0.0000e+00,\n",
      "         6.3964e+00, 2.4759e+00, 7.3591e+00, -0.0000e+00, 1.4306e+01, 7.2018e+00,\n",
      "         -0.0000e+00, 1.5418e-01, 1.3316e+01, 6.7762e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 8.5292e-02, -0.0000e+00, 6.7308e+00, 7.3369e+00,\n",
      "         6.0749e+00, -0.0000e+00, -0.0000e+00, 5.5071e+00, 1.2546e+01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0833e+01, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.6742e+00, 1.5455e+01,\n",
      "         4.0180e-01, -0.0000e+00, 1.9562e+01, -0.0000e+00, -0.0000e+00, 6.2697e+00,\n",
      "         6.9085e-01, -0.0000e+00, 1.5884e+01, -0.0000e+00, -0.0000e+00, 1.0930e+01,\n",
      "         8.5606e+00, 3.2145e+00, 4.8603e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 5.9063e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.6312e+00,\n",
      "         4.7565e+00, 1.4524e+01, 6.9942e+00, 3.1497e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, 9.4110e-01, 4.6452e-01, -0.0000e+00, 1.2260e+00,\n",
      "         -0.0000e+00, 1.2387e+00, -0.0000e+00, 5.8240e+00, 1.4314e+00, 1.5665e-02,\n",
      "         -0.0000e+00, 2.7741e+00, 1.4823e+00, 2.4993e+00, 2.8947e+00, 2.4497e-01,\n",
      "         3.5366e-01, 3.5338e+00, 8.6683e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         4.2097e-01, 1.2407e+00, -0.0000e+00, 3.8904e+00, 4.1279e+00, -0.0000e+00,\n",
      "         4.2011e+00, -0.0000e+00, 3.7996e+00, 6.0914e+00, 2.2199e+00, 3.4800e-01,\n",
      "         1.5258e+00, -0.0000e+00, -0.0000e+00, 3.6636e+00, 2.5121e+00, -0.0000e+00,\n",
      "         2.2327e+00, 2.0069e+00, 2.1567e+00, -0.0000e+00, 3.4567e-01, 1.5012e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 7.7084e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4894e+00, 7.7262e-01,\n",
      "         -0.0000e+00, 5.2563e+00, 1.7720e+00, 1.8992e+00, -0.0000e+00, -0.0000e+00,\n",
      "         2.7041e+00, -0.0000e+00, 1.7160e+00, 2.4378e+00, -0.0000e+00, -0.0000e+00,\n",
      "         4.4906e+00, 3.2371e+00, -0.0000e+00, -0.0000e+00, 5.8329e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 1.4776e+00, 5.1365e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.4874e+00, -0.0000e+00, 3.1045e+00, -0.0000e+00, 8.6296e-01, 6.4253e-01,\n",
      "         2.4271e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.3113e+00,\n",
      "         4.9117e-01, -0.0000e+00, 3.7147e+00, 1.2229e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, 8.7858e+00, -0.0000e+00, 8.5716e+00,\n",
      "         -0.0000e+00, 3.9947e-01, 1.0892e+00, 9.6485e+00, 1.1670e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 1.8465e+01, 1.0032e+01, -0.0000e+00, 3.8018e+00, 9.7955e-01,\n",
      "         8.9155e+00, -0.0000e+00, 1.3050e+01, -0.0000e+00, 4.0129e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.8070e+01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         4.9304e+00, 1.4162e+01, -0.0000e+00, 2.0354e+00, -0.0000e+00, -0.0000e+00,\n",
      "         9.8546e+00, -0.0000e+00, 2.0261e+00, -0.0000e+00, 2.6689e+00, 9.1683e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.5533e+00,\n",
      "         1.0448e+01, -0.0000e+00, 9.1680e+00, 5.6450e-01, 6.0077e+00, 4.7197e-01,\n",
      "         1.7796e+01, 1.0623e+01, 6.9414e+00, -0.0000e+00, 1.5944e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.2683e+00, 1.0203e+01, 4.7711e+00,\n",
      "         1.2723e+01, -0.0000e+00, 7.6521e-01, -0.0000e+00, 3.5686e+00, -0.0000e+00,\n",
      "         2.6204e+00, 8.6827e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.2651e-01,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.6342e+00, 1.0878e+01,\n",
      "         -0.0000e+00, -0.0000e+00, 9.4613e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         4.2346e+00, 1.1708e+01, -0.0000e+00, -0.0000e+00],\n",
      "        [1.0831e+00, 8.9698e-01, 6.5717e+00, -0.0000e+00, -0.0000e+00, 4.4136e+00,\n",
      "         3.2100e-01, 3.9852e+00, 6.0793e+00, 7.0574e+00, 3.7327e-01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4503e+00, 1.8908e+00, 2.9727e+00,\n",
      "         5.4093e+00, 3.0578e+00, -0.0000e+00, 1.6358e+00, 3.5737e-01, 2.2221e+00,\n",
      "         6.1709e+00, 3.8852e+00, -0.0000e+00, 3.4943e+00, -0.0000e+00, 3.9872e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0213e+01, 8.5574e-02, 2.4362e+00,\n",
      "         1.5676e+00, 2.2290e+00, -0.0000e+00, 1.1446e+01, -0.0000e+00, 1.4947e+00,\n",
      "         5.7361e+00, -0.0000e+00, 5.8935e-01, 1.7086e+00, 3.2911e+00, -0.0000e+00,\n",
      "         1.2842e+01, 1.1785e-02, 7.8626e+00, 3.2170e+00, 4.9349e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1434e+00, 5.1213e+00, -0.0000e+00,\n",
      "         9.3529e-01, 9.3185e+00, -0.0000e+00, 4.1187e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 2.3224e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3781e-01,\n",
      "         6.5539e+00, 9.6905e+00, 1.8005e+00, -0.0000e+00, 9.1528e-01, 8.7412e-01,\n",
      "         -0.0000e+00, 1.0838e+00, 8.6912e+00, 2.0536e-02, 2.4940e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 6.7787e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 1.3828e+00, -0.0000e+00, -0.0000e+00, 5.4891e-01, 3.0707e+00,\n",
      "         -0.0000e+00, 1.5336e+00, 1.7401e+00, -0.0000e+00],\n",
      "        [3.0083e+00, 2.4453e+00, -0.0000e+00, 1.0326e+01, -0.0000e+00, 3.1097e+00,\n",
      "         -0.0000e+00, 6.3590e+00, -0.0000e+00, 1.4913e+01, -0.0000e+00, 5.7088e+00,\n",
      "         -0.0000e+00, 1.0098e+01, 8.3631e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         5.4983e+00, -0.0000e+00, -0.0000e+00, 4.4363e-01, -0.0000e+00, 4.1593e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 6.1567e+00, -0.0000e+00, 1.5146e+01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 7.3319e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 1.1073e+01, 7.6225e+00, -0.0000e+00, 9.4524e-01, -0.0000e+00,\n",
      "         5.5566e+00, -0.0000e+00, 1.2904e+01, -0.0000e+00, -0.0000e+00, 4.3142e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.6699e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 1.0746e+01, 6.4081e+00, 1.0505e+01, -0.0000e+00,\n",
      "         6.9492e+00, 5.1736e-01, -0.0000e+00, 1.5072e+01, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 2.0968e+00, 8.3799e-01, -0.0000e+00, 1.5937e+01, 1.5282e+01,\n",
      "         1.0035e+01, 1.7054e-01, 7.1386e+00, -0.0000e+00, -0.0000e+00, 2.7209e+00,\n",
      "         6.0506e+00, 7.0846e+00, 6.1534e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.3016e+01, 1.8540e-01, -0.0000e+00, -0.0000e+00, 3.9773e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 5.4758e+00, 5.0833e+00, -0.0000e+00, 1.0582e+00,\n",
      "         1.3938e+01, 1.5094e+01, -0.0000e+00, -0.0000e+00],\n",
      "        [1.0508e+01, 9.2356e+00, 1.1754e+01, -0.0000e+00, -0.0000e+00, 1.9876e+01,\n",
      "         -0.0000e+00, 1.3212e+01, -0.0000e+00, 5.1195e+01, -0.0000e+00, 2.1947e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 2.2371e+00, 6.2011e-01, 1.9095e+01, -0.0000e+00,\n",
      "         -0.0000e+00, 2.9043e+00, -0.0000e+00, -0.0000e+00, 7.6419e+00, -0.0000e+00,\n",
      "         8.7138e+00, 1.0186e+01, -0.0000e+00, 1.8207e+01, 1.3772e+01, 1.4604e+01,\n",
      "         -0.0000e+00, -0.0000e+00, 3.0491e+01, 4.2869e+01, 4.3999e+00, 7.5345e+00,\n",
      "         6.8215e+00, 3.4295e-01, -0.0000e+00, 1.5040e+01, 1.7875e+01, 1.9655e+01,\n",
      "         1.0423e+01, -0.0000e+00, 2.3156e+01, 1.4709e+01, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 1.5943e+01, -0.0000e+00, 7.3558e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.2231e+00, 1.3302e+01, -0.0000e+00,\n",
      "         -0.0000e+00, 1.4620e+01, -0.0000e+00, 2.1848e+01, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 8.2295e+00, 2.8306e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         2.0922e+01, 2.5906e+01, 1.1865e+01, -0.0000e+00, 1.2796e+01, 2.6991e+00,\n",
      "         -0.0000e+00, 1.5511e+01, 3.8542e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.1213e+01, -0.0000e+00, 3.4670e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.5852e+01, 1.5617e+01, 2.3100e+00, -0.0000e+00, -0.0000e+00, 2.4803e+01,\n",
      "         -0.0000e+00, 2.9240e+00, 2.6310e+01, -0.0000e+00],\n",
      "        [2.1743e+01, 1.1568e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.6357e+00,\n",
      "         -0.0000e+00, 3.3187e+00, 9.8296e+00, 2.5924e+01, -0.0000e+00, 2.1595e+01,\n",
      "         -0.0000e+00, 8.1363e+00, 1.8216e+01, -0.0000e+00, 7.3511e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.5491e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 9.6031e-01, 2.9460e+00, -0.0000e+00, 7.4381e+00, 4.8976e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 2.0392e+01, 2.0195e+01, -0.0000e+00, 3.3229e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3177e+01, 2.6961e+01,\n",
      "         1.8766e-01, -0.0000e+00, 1.7245e+01, -0.0000e+00, -0.0000e+00, 8.4170e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.5696e+00, 1.6438e+01,\n",
      "         -0.0000e+00, 6.9691e-01, -0.0000e+00, 1.9186e+01, 1.0690e+01, 3.4556e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0200e+01, -0.0000e+00, 2.9807e+00,\n",
      "         -0.0000e+00, 7.7939e+00, -0.0000e+00, -0.0000e+00, 1.7042e+01, 2.2269e+01,\n",
      "         1.1747e+01, -0.0000e+00, 2.9627e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.0959e+01, 1.1031e+01, 2.6327e+01, -0.0000e+00, -0.0000e+00, 2.4784e+01,\n",
      "         2.0174e+01, 5.6248e+00, 8.3498e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 3.4704e+00, -0.0000e+00, 1.1273e+01, -0.0000e+00, 1.3292e+01,\n",
      "         1.9456e+00, 7.1487e+00, 1.1102e+01, 7.9482e+00],\n",
      "        [-0.0000e+00, 2.1126e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         2.3547e+00, -0.0000e+00, 1.2026e+01, 5.4803e+00, 9.4377e+00, 3.2700e+00,\n",
      "         4.5710e+00, 4.6539e+00, 1.1257e+01, -0.0000e+00, 5.4046e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 4.4616e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 2.3781e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.5857e+00,\n",
      "         -0.0000e+00, 1.0031e+01, -0.0000e+00, 8.0624e+00, 1.9574e+00, 1.0115e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.3868e+00, 1.4072e-01, 1.0809e+01,\n",
      "         2.5299e+00, -0.0000e+00, 4.5925e+00, -0.0000e+00, 3.7816e+00, 1.6213e+01,\n",
      "         1.1327e+01, 2.7087e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.7999e+00,\n",
      "         -0.0000e+00, 9.7801e+00, -0.0000e+00, 9.5755e+00, 1.1238e+00, 8.7177e+00,\n",
      "         5.5170e+00, -0.0000e+00, 9.8775e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 7.1663e+00, -0.0000e+00, 4.1622e+00, 1.2876e+01, 6.7342e+00,\n",
      "         1.4723e+01, -0.0000e+00, 1.1150e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         7.3012e+00, 7.1630e+00, 1.1480e+01, -0.0000e+00, -0.0000e+00, 1.8867e+01,\n",
      "         9.3488e+00, -0.0000e+00, 2.4176e+00, -0.0000e+00, 5.7326e+00, 1.1667e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6063e+01, -0.0000e+00, 3.2680e+00,\n",
      "         4.4030e+00, -0.0000e+00, -0.0000e+00, 7.9929e+00]])\n",
      "\n",
      "Second weight layer outputs: tensor([[2.0534e+01, 5.7450e+00, -0.0000e+00, 7.5047e+00, -0.0000e+00, 5.2946e+00,\n",
      "         -0.0000e+00, 1.6029e+00, 2.8609e+00, 1.6004e+01, -0.0000e+00, 1.6897e+01,\n",
      "         -0.0000e+00, 9.3483e+00, 6.5865e+00, -0.0000e+00, 3.0678e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.6840e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 4.4081e+00, -0.0000e+00, 1.1176e+01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 1.7439e+01, 1.2983e+01, -0.0000e+00, -0.0000e+00,\n",
      "         6.3964e+00, 2.4759e+00, 7.3591e+00, -0.0000e+00, 1.4306e+01, 7.2018e+00,\n",
      "         -0.0000e+00, 1.5418e-01, 1.3316e+01, 6.7762e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 8.5292e-02, -0.0000e+00, 6.7308e+00, 7.3369e+00,\n",
      "         6.0749e+00, -0.0000e+00, -0.0000e+00, 5.5071e+00, 1.2546e+01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0833e+01, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.6742e+00, 1.5455e+01,\n",
      "         4.0180e-01, -0.0000e+00, 1.9562e+01, -0.0000e+00, -0.0000e+00, 6.2697e+00,\n",
      "         6.9085e-01, -0.0000e+00, 1.5884e+01, -0.0000e+00, -0.0000e+00, 1.0930e+01,\n",
      "         8.5606e+00, 3.2145e+00, 4.8603e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 5.9063e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.6312e+00,\n",
      "         4.7565e+00, 1.4524e+01, 6.9942e+00, 3.1497e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, 9.4110e-01, 4.6452e-01, -0.0000e+00, 1.2260e+00,\n",
      "         -0.0000e+00, 1.2387e+00, -0.0000e+00, 5.8240e+00, 1.4314e+00, 1.5665e-02,\n",
      "         -0.0000e+00, 2.7741e+00, 1.4823e+00, 2.4993e+00, 2.8947e+00, 2.4497e-01,\n",
      "         3.5366e-01, 3.5338e+00, 8.6683e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         4.2097e-01, 1.2407e+00, -0.0000e+00, 3.8904e+00, 4.1279e+00, -0.0000e+00,\n",
      "         4.2011e+00, -0.0000e+00, 3.7996e+00, 6.0914e+00, 2.2199e+00, 3.4800e-01,\n",
      "         1.5258e+00, -0.0000e+00, -0.0000e+00, 3.6636e+00, 2.5121e+00, -0.0000e+00,\n",
      "         2.2327e+00, 2.0069e+00, 2.1567e+00, -0.0000e+00, 3.4567e-01, 1.5012e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 7.7084e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4894e+00, 7.7262e-01,\n",
      "         -0.0000e+00, 5.2563e+00, 1.7720e+00, 1.8992e+00, -0.0000e+00, -0.0000e+00,\n",
      "         2.7041e+00, -0.0000e+00, 1.7160e+00, 2.4378e+00, -0.0000e+00, -0.0000e+00,\n",
      "         4.4906e+00, 3.2371e+00, -0.0000e+00, -0.0000e+00, 5.8329e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 1.4776e+00, 5.1365e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.4874e+00, -0.0000e+00, 3.1045e+00, -0.0000e+00, 8.6296e-01, 6.4253e-01,\n",
      "         2.4271e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.3113e+00,\n",
      "         4.9117e-01, -0.0000e+00, 3.7147e+00, 1.2229e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, 8.7858e+00, -0.0000e+00, 8.5716e+00,\n",
      "         -0.0000e+00, 3.9947e-01, 1.0892e+00, 9.6485e+00, 1.1670e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 1.8465e+01, 1.0032e+01, -0.0000e+00, 3.8018e+00, 9.7955e-01,\n",
      "         8.9155e+00, -0.0000e+00, 1.3050e+01, -0.0000e+00, 4.0129e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.8070e+01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         4.9304e+00, 1.4162e+01, -0.0000e+00, 2.0354e+00, -0.0000e+00, -0.0000e+00,\n",
      "         9.8546e+00, -0.0000e+00, 2.0261e+00, -0.0000e+00, 2.6689e+00, 9.1683e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.5533e+00,\n",
      "         1.0448e+01, -0.0000e+00, 9.1680e+00, 5.6450e-01, 6.0077e+00, 4.7197e-01,\n",
      "         1.7796e+01, 1.0623e+01, 6.9414e+00, -0.0000e+00, 1.5944e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.2683e+00, 1.0203e+01, 4.7711e+00,\n",
      "         1.2723e+01, -0.0000e+00, 7.6521e-01, -0.0000e+00, 3.5686e+00, -0.0000e+00,\n",
      "         2.6204e+00, 8.6827e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.2651e-01,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.6342e+00, 1.0878e+01,\n",
      "         -0.0000e+00, -0.0000e+00, 9.4613e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         4.2346e+00, 1.1708e+01, -0.0000e+00, -0.0000e+00],\n",
      "        [1.0831e+00, 8.9698e-01, 6.5717e+00, -0.0000e+00, -0.0000e+00, 4.4136e+00,\n",
      "         3.2100e-01, 3.9852e+00, 6.0793e+00, 7.0574e+00, 3.7327e-01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4503e+00, 1.8908e+00, 2.9727e+00,\n",
      "         5.4093e+00, 3.0578e+00, -0.0000e+00, 1.6358e+00, 3.5737e-01, 2.2221e+00,\n",
      "         6.1709e+00, 3.8852e+00, -0.0000e+00, 3.4943e+00, -0.0000e+00, 3.9872e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0213e+01, 8.5574e-02, 2.4362e+00,\n",
      "         1.5676e+00, 2.2290e+00, -0.0000e+00, 1.1446e+01, -0.0000e+00, 1.4947e+00,\n",
      "         5.7361e+00, -0.0000e+00, 5.8935e-01, 1.7086e+00, 3.2911e+00, -0.0000e+00,\n",
      "         1.2842e+01, 1.1785e-02, 7.8626e+00, 3.2170e+00, 4.9349e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1434e+00, 5.1213e+00, -0.0000e+00,\n",
      "         9.3529e-01, 9.3185e+00, -0.0000e+00, 4.1187e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 2.3224e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3781e-01,\n",
      "         6.5539e+00, 9.6905e+00, 1.8005e+00, -0.0000e+00, 9.1528e-01, 8.7412e-01,\n",
      "         -0.0000e+00, 1.0838e+00, 8.6912e+00, 2.0536e-02, 2.4940e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 6.7787e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 1.3828e+00, -0.0000e+00, -0.0000e+00, 5.4891e-01, 3.0707e+00,\n",
      "         -0.0000e+00, 1.5336e+00, 1.7401e+00, -0.0000e+00],\n",
      "        [3.0083e+00, 2.4453e+00, -0.0000e+00, 1.0326e+01, -0.0000e+00, 3.1097e+00,\n",
      "         -0.0000e+00, 6.3590e+00, -0.0000e+00, 1.4913e+01, -0.0000e+00, 5.7088e+00,\n",
      "         -0.0000e+00, 1.0098e+01, 8.3631e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         5.4983e+00, -0.0000e+00, -0.0000e+00, 4.4363e-01, -0.0000e+00, 4.1593e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 6.1567e+00, -0.0000e+00, 1.5146e+01, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 7.3319e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 1.1073e+01, 7.6225e+00, -0.0000e+00, 9.4524e-01, -0.0000e+00,\n",
      "         5.5566e+00, -0.0000e+00, 1.2904e+01, -0.0000e+00, -0.0000e+00, 4.3142e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.6699e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 1.0746e+01, 6.4081e+00, 1.0505e+01, -0.0000e+00,\n",
      "         6.9492e+00, 5.1736e-01, -0.0000e+00, 1.5072e+01, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 2.0968e+00, 8.3799e-01, -0.0000e+00, 1.5937e+01, 1.5282e+01,\n",
      "         1.0035e+01, 1.7054e-01, 7.1386e+00, -0.0000e+00, -0.0000e+00, 2.7209e+00,\n",
      "         6.0506e+00, 7.0846e+00, 6.1534e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.3016e+01, 1.8540e-01, -0.0000e+00, -0.0000e+00, 3.9773e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 5.4758e+00, 5.0833e+00, -0.0000e+00, 1.0582e+00,\n",
      "         1.3938e+01, 1.5094e+01, -0.0000e+00, -0.0000e+00],\n",
      "        [1.0508e+01, 9.2356e+00, 1.1754e+01, -0.0000e+00, -0.0000e+00, 1.9876e+01,\n",
      "         -0.0000e+00, 1.3212e+01, -0.0000e+00, 5.1195e+01, -0.0000e+00, 2.1947e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 2.2371e+00, 6.2011e-01, 1.9095e+01, -0.0000e+00,\n",
      "         -0.0000e+00, 2.9043e+00, -0.0000e+00, -0.0000e+00, 7.6419e+00, -0.0000e+00,\n",
      "         8.7138e+00, 1.0186e+01, -0.0000e+00, 1.8207e+01, 1.3772e+01, 1.4604e+01,\n",
      "         -0.0000e+00, -0.0000e+00, 3.0491e+01, 4.2869e+01, 4.3999e+00, 7.5345e+00,\n",
      "         6.8215e+00, 3.4295e-01, -0.0000e+00, 1.5040e+01, 1.7875e+01, 1.9655e+01,\n",
      "         1.0423e+01, -0.0000e+00, 2.3156e+01, 1.4709e+01, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 1.5943e+01, -0.0000e+00, 7.3558e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.2231e+00, 1.3302e+01, -0.0000e+00,\n",
      "         -0.0000e+00, 1.4620e+01, -0.0000e+00, 2.1848e+01, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 8.2295e+00, 2.8306e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         2.0922e+01, 2.5906e+01, 1.1865e+01, -0.0000e+00, 1.2796e+01, 2.6991e+00,\n",
      "         -0.0000e+00, 1.5511e+01, 3.8542e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.1213e+01, -0.0000e+00, 3.4670e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.5852e+01, 1.5617e+01, 2.3100e+00, -0.0000e+00, -0.0000e+00, 2.4803e+01,\n",
      "         -0.0000e+00, 2.9240e+00, 2.6310e+01, -0.0000e+00],\n",
      "        [2.1743e+01, 1.1568e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.6357e+00,\n",
      "         -0.0000e+00, 3.3187e+00, 9.8296e+00, 2.5924e+01, -0.0000e+00, 2.1595e+01,\n",
      "         -0.0000e+00, 8.1363e+00, 1.8216e+01, -0.0000e+00, 7.3511e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.5491e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 9.6031e-01, 2.9460e+00, -0.0000e+00, 7.4381e+00, 4.8976e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 2.0392e+01, 2.0195e+01, -0.0000e+00, 3.3229e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3177e+01, 2.6961e+01,\n",
      "         1.8766e-01, -0.0000e+00, 1.7245e+01, -0.0000e+00, -0.0000e+00, 8.4170e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.5696e+00, 1.6438e+01,\n",
      "         -0.0000e+00, 6.9691e-01, -0.0000e+00, 1.9186e+01, 1.0690e+01, 3.4556e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0200e+01, -0.0000e+00, 2.9807e+00,\n",
      "         -0.0000e+00, 7.7939e+00, -0.0000e+00, -0.0000e+00, 1.7042e+01, 2.2269e+01,\n",
      "         1.1747e+01, -0.0000e+00, 2.9627e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         1.0959e+01, 1.1031e+01, 2.6327e+01, -0.0000e+00, -0.0000e+00, 2.4784e+01,\n",
      "         2.0174e+01, 5.6248e+00, 8.3498e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 3.4704e+00, -0.0000e+00, 1.1273e+01, -0.0000e+00, 1.3292e+01,\n",
      "         1.9456e+00, 7.1487e+00, 1.1102e+01, 7.9482e+00],\n",
      "        [-0.0000e+00, 2.1126e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         2.3547e+00, -0.0000e+00, 1.2026e+01, 5.4803e+00, 9.4377e+00, 3.2700e+00,\n",
      "         4.5710e+00, 4.6539e+00, 1.1257e+01, -0.0000e+00, 5.4046e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, 4.4616e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 2.3781e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.5857e+00,\n",
      "         -0.0000e+00, 1.0031e+01, -0.0000e+00, 8.0624e+00, 1.9574e+00, 1.0115e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.3868e+00, 1.4072e-01, 1.0809e+01,\n",
      "         2.5299e+00, -0.0000e+00, 4.5925e+00, -0.0000e+00, 3.7816e+00, 1.6213e+01,\n",
      "         1.1327e+01, 2.7087e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.7999e+00,\n",
      "         -0.0000e+00, 9.7801e+00, -0.0000e+00, 9.5755e+00, 1.1238e+00, 8.7177e+00,\n",
      "         5.5170e+00, -0.0000e+00, 9.8775e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, 7.1663e+00, -0.0000e+00, 4.1622e+00, 1.2876e+01, 6.7342e+00,\n",
      "         1.4723e+01, -0.0000e+00, 1.1150e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         7.3012e+00, 7.1630e+00, 1.1480e+01, -0.0000e+00, -0.0000e+00, 1.8867e+01,\n",
      "         9.3488e+00, -0.0000e+00, 2.4176e+00, -0.0000e+00, 5.7326e+00, 1.1667e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6063e+01, -0.0000e+00, 3.2680e+00,\n",
      "         4.4030e+00, -0.0000e+00, -0.0000e+00, 7.9929e+00]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input((8,20))\n",
    "\n",
    "print('Input layer done')\n",
    "w_1 = Input((20,10))\n",
    "print('Weight 1 layer done')\n",
    "\n",
    "\n",
    "b_1 = Input((1, 10))\n",
    "print('Bias 1 layer done')\n",
    "w_2 = Input((10, 100))\n",
    "print('weight 2 layer done')\n",
    "b_2 = Input((1, 100))\n",
    "print('bias 2 layer done')\n",
    "print('\\n\\n\\n')\n",
    "w_layer_1 = Linear(input_layer, (1,10), w_1, bias=b_1)\n",
    "w_layer_2 = Linear(w_layer_1, (1,100), w_2, bias=b_2) \n",
    "print(f'\\nInput layer values: {input_layer.values}\\n')\n",
    "w_layer_1.forward()\n",
    "print(f'\\nFirst weight layer outputs: {w_layer_1.values}\\n')\n",
    "w_layer_2.forward()\n",
    "print(f'\\nSecond weight layer outputs: {w_layer_2.values}\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arbitrary network and batch sizes work :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this lab... except for the two **required** parting questions:\n",
    "\n",
    "### <font color=blue>Question 6: Summarize what you learned during this lab.\n",
    "\n",
    "Ans: The main thing I learned in this lab is the forward pass of a neural network. Besides this I was also able to refresh my pytorch skills including using tensors and creating various tensors as well. I also had to learn matrix operations using pytorch which will also be useful for the future. Lastly, this lab also allowed me to refresh my Linear Algebra skills. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 7: Describe what you liked about this lab *or* what could be improved. (Required.)\n",
    "\n",
    "Ans: One thing that would be useful to add to this lab would be to really hammer down the point that even weights and biases are supposed to be their own layer. Many students, I believe, tend to think of the weights, bias and activation function as one complete layer. This definitely requires some adjusting and could be reenforced in the lab as well. I really enjoyed that the lab made you walk through it by hand and then programatically. Having the previous test case when testing the classes was super useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
